% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[preprint]{acl}
%\usepackage[review]{acl}
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}    % For including images
\usepackage{inconsolata}

\usepackage{booktabs} %for tables

\usepackage{float}

\usepackage{siunitx}

\usepackage{tabularx}

\usepackage{booktabs}

\usepackage{amsmath}


\usepackage{subcaption}  % For subfigures
\usepackage{caption}     % Fine-tuning captions
\usepackage{adjustbox}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{Instructions for *ACL Proceedings}

%\title{Large Language Models as Mirrors of Societal Moral Standards}

\title{Exploring Cultural Variations in Moral Judgments \\with Large Language Models
%: Reflecting Cultural Divergence Across Ethical Topics
}

\author{Hadi Mohammadi, Efthymia Papadopoulou, Yasmeen F.S.S. Meijer,\and Ayoub Bagheri \\
Department of Methodology and Statistics, Utrecht University, Utrecht, The Netherlands \\
\texttt{h.mohammadi@uu.nl, evi.papado98@gmail.com, mijntje.meijer@live.nl} \\
\texttt{,and a.bagheri@uu.nl}}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) have shown strong performance across many tasks, but their ability to capture culturally diverse moral values remains unclear. In this paper, we examine whether LLMs can mirror variations in moral attitudes reported by two major cross-cultural surveys: the World Values Survey and the PEW Research Center’s Global Attitudes Survey. We compare smaller, monolingual, and multilingual models (GPT-2, OPT, BLOOMZ, and Qwen) with more recent instruction-tuned models (GPT-4o, GPT-4o-mini, Gemma-2-9b-it, and Llama-3.3-70B-Instruct). Using log-probability-based~\emph{moral justifiability} scores, we correlate each model’s outputs with survey data covering a broad set of ethical topics. Our results show that many earlier or smaller models often produce near-zero or negative correlations with human judgments. In contrast, advanced instruction-tuned models (including GPT-4o and GPT-4o-mini) achieve substantially higher positive correlations, suggesting they better reflect real-world moral attitudes. While scaling up model size and using instruction tuning can improve alignment with cross-cultural moral norms, challenges remain for certain topics and regions. We discuss these findings in relation to bias analysis, training data diversity, and strategies for improving the cultural sensitivity of LLMs.
\end{abstract}


\section{Introduction}
Over the past few years, LLMs have gained prominence in both academic and public discussions~\citep{Bender2021}. Advances in model performance have made LLMs appealing for diverse applications, such as social media content moderation, chatbots, content creation, real-time translation, search engines, recommendation systems, and automated decision-making. While modern LLMs (e.g., GPT-4) show strong performance, a critical concern is how these models may inherit biases, including gender, racial, or cultural biases, from their training data. LLMs can easily absorb such biases because they learn from large-scale text corpora containing entrenched stereotypes~\citep{Staczak2021, karpouzis2024}.
%mishra2024 

These biases raise concerns about fairness, particularly in contexts requiring moral judgments. If an LLM is trained mostly on data that negatively or inaccurately portrays certain cultural groups, it may repeat that bias in its responses. As these models become more widespread and globally deployed, the risk of perpetuating cultural biases grows, especially when moral perspectives are different from common rules or what surveys usually show. In fact, recent research shows that current LLMs often exhibit a default Western-centric bias~\citep{adilazuarda2024towards}, underscoring the need to evaluate their cross-cultural validity



It is crucial to see whether LLMs accurately mirror the moral judgments observed across diverse cultures. Despite its importance, this issue has received limited attention~\citep{arora2023probing, liu2024multilingual}. Our study investigates whether both monolingual and multilingual Pre-trained Language Models (PLMs) can capture nuanced cultural norms. These norms include subtle ethical differences across regions, for example, the acceptance of alcohol consumption or differing attitudes on topics like abortion. Although recent research suggests that multilingual PLMs might capture broader cultural nuances, they often fall short of reflecting the moral subtleties present in less dominant cultural groups~\citep{Hmmerl2022, papadopoulou2024}.

We examine this question using two well-known cross-cultural datasets: the World Values Survey (WVS)~\citep{Inglehart2014, Haerpfer2022}, and the PEW Research Center’s Global Attitudes Survey, which includes a module on moral issues across many countries~\citep{Pew2023}. These surveys offer a detailed view of moral and cultural norms globally, serving as a benchmark for comparing LLMs outputs against actual human responses. By converting survey questions into prompts, we derive log-probability-based~\emph{moral justifiability} scores. We then compare these scores with survey-based consensus on various ethical issues (e.g., drinking alcohol, sex before marriage, abortion, homosexuality), allowing us to see how closely different model types and training approaches align with cultural norms. Evaluating how effectively LLMs represent cultural values has both scholarly and practical significance. If a model systematically misrepresents or overlooks certain moral perspectives, it may reinforce stereotypes or lead to biased outcomes. On the other hand, more culturally aware models can highlight both shared values and nuanced disagreements, potentially contributing to more balanced dialogue. By comparing model outputs to reliable survey data, we identify areas where LLMs align with human values and highlight gaps in capturing diverse moral perspectives.

Our contributions are threefold: (1) We introduce a structured probing framework that leverages carefully designed prompts, contrasting moral statements, and log-probability-based scoring to assess how LLMs assign~\emph{justifiability} values to morally complex scenarios across cultures. (2) We empirically analyze the alignment between LLM-derived moral scores and human survey responses using correlation and clustering, highlighting where models reflect or deviate from real-world moral judgments. (3) We extend our evaluation to state-of-the-art instruction-tuned and large-scale models, examining whether instruction tuning and scaling enhance alignment with cross-cultural moral norms. By identifying key strengths, weaknesses, and factors influencing model-human agreement, our work contributes to improving training data strategies, mitigating biases, and fostering the development of culturally aware language models. 


\section{Literature review}
%LLMs inherit biases present in their training data, and these biases can sometimes be amplified. Since LLMs are trained on extensive text corpora that reflect societal and cultural influences, they inevitably learn patterns that may reinforce existing disparities. This has raised concerns about fairness, representation, and the broader implications of deploying LLMs in real-world applications~\citep{Bender2021}.Moral judgments refer to evaluations of actions, intentions, or individuals as either acceptable or objectionable. These judgments vary widely by culture, shaped by religion, social norms, and historical factors~\citep{Haidt2001, Shweder1997}. 


%\paragraph{Biases and Moral Judgments in LLMs.}
LLMs inherit biases embedded in their training data, and these biases can be amplified upon large-scale deployment. Because the underlying corpora often reflect entrenched social hierarchies, models run the risk of reproducing or even intensifying unfair patterns. Recent work has underscored this from multiple perspectives, a 2025 study introduced a unified framework for transparency, fairness, and privacy in AI pipelines~\citep{Radanliev2025}, while an interdisciplinary survey emphasized the importance of \emph{diversity, equity, and inclusion (DEI)} as prerequisites for trustworthy AI~\citep{cachat2023diversity}. Taken together with earlier warnings about opaque language-model behaviors~\citep{Bender2021}, these findings illustrate the need for technical innovation to proceed hand-in-hand with social safeguards. In addition to high-level ethical governance, researchers are exploring concrete mitigation strategies. For example, LLM data augmentation has improved intent-classification accuracy without sacrificing fairness, provided that the augmentation is carefully curated~\citep{benayas2024enhancing}. Complementary work on adapter tuning for non-English LLMs shows that modest architectural modifications can substantially boost performance in culturally diverse benchmarks, thereby supporting more inclusive NLP systems~\citep{Zhou2024}.

Moral judgments themselves, evaluations of actions, intentions, or individuals as acceptable or objectionable, can differ widely by culture, shaped by religious traditions, social norms, and historical contexts~\citep{Haidt2001, Shweder1997}. Understanding how such pluralistic values are (or are not) embedded in contemporary LLMs remains a pressing research concern.  As noted by~\citet{Graham2016}, Western, Educated, Industrialized, Rich, and Democratic (W.E.I.R.D.) societies emphasize individual rights and autonomy, while non-W.E.I.R.D. societies often stress communal responsibilities and spiritual considerations. Consequently, people in W.E.I.R.D. cultures may view personal choices like sexual behavior as an individual right, while those in non-W.E.I.R.D. cultures consider them a collective moral concern. Although many moral values overlap across cultures, there are also areas of genuine divergence, often referred to as \emph{moral value pluralism}~\citep{Johnson2022, Benkler2023}. However,~\citet{Kharchenko2024} argue that LLMs struggle to capture pluralistic moral values because their training data lacks sufficient cultural variety. Likewise,~\citet{Du2024} point out that the heavy use of English data in LLMs training limits the representation and creativity of models in other languages, although larger training corpora and bigger model architectures can improve performance.~\citet{arora2023probing} suggest that multilingual LLMs could learn cultural values by incorporating multilingual data in their training. Yet, the limited diversity within multilingual corpora can still cause these models to perform inconsistently across languages and cultural contexts.~\citet{Benkler2023} emphasize that many current AI systems lean toward the dominant values of Western cultures, especially English-speaking ones, leading to an implicit assumption that W.E.I.R.D. values are universal.

During training, LLMs use word embeddings to learn semantic and syntactic relationships based on how frequently words co-occur. These embeddings can encode the same social biases found in the training data~\citep{nemani2024gender, mohammadi2025explainability}. This association-based learning can produce biased outputs that influence the model’s fairness and reliability. For instance, ~\citet{Johnson2022} showed that GPT-3 used the term \emph{Muslims} in violent contexts more often than \emph{Christians}, reinforcing damaging stereotypes. In all these cases, biased outputs can influence public perceptions and decisions, highlighting the importance of bias detection and mitigation~\citep{Noble2018, Zou2018}.
%

Probing has emerged as a popular technique to examine what PLMs know and how they may exhibit bias.~\citet{Ousidhoum2021} used probing to detect hateful or toxic content toward specific communities, while~\citet{nadeem2021stereoset} used context-based association tests to investigate stereotypes.~\citet{arora2023probing} adapted cross-cultural survey questions into prompts to test multilingual PLMs in 13 languages, discovering that these models often failed to match the moral values embedded in their training languages. Although there are multiple probing approaches, from \emph{cloze-style} tasks to \emph{pseudo-log-likelihood} scoring~\citep{nadeem2021stereoset, Salazar2019}, each has limitations. A simpler method directly computes the probability of specific tokens, following the original transformer design~\citep{Vaswani2017}.

Research on AI ethics underscores the need for models that respect cultural distinctions and support equitable treatment~\citep{ Zowghi2023, CachatRosset2023, karpouzis2024, meijer2024}. Yet, biases in training data or architectural choices can lead to inconsistent handling of inputs from various backgrounds, raising doubts about an AI system’s fairness and applicability~\citep{karpouzis2024}.While studies like~\citet{arora2023probing} and \citet{Benkler2023} find that LLMs often struggle to accurately reflect diverse moral perspectives, others such as~\citet{ramezani2023knowledge} indicate that LLMs can sometimes capture considerable cultural variety. Similarly,~\citet{cao2023assessing} showed that ChatGPT aligns strongly with American cultural norms while adapting less effectively to others, reinforcing concerns of Western-centric bias in LLM outputs. This discrepancy highlights the need for more research on how LLMs learn and represent moral values in different cultural settings. Even though LLMs can inherit some cultural biases, the extent of their cross-cultural fidelity remains an open question~\citep{caliskan2017semantics}.

\section{Materials and methods}
\subsection{Data}
\label{sec:data}

To evaluate cross-cultural moral attitudes, we use two datasets: World Values Survey (WVS) Wave~7 and the PEW Research Center Global Attitudes Survey 2013. Each dataset’s moral questions are labeled with topic codes. See Table~\ref{tab:topic_mapping} in Appendix~\ref{appendix:topic_codes} for a full reference.

\paragraph{World Values Survey Wave 7} The WVS conducted from 2017 to 2020\footnote{\url{https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp}}, which covers respondents from 55 countries~\citep{Inglehart2014, Haerpfer2022}. We use the section of the survey dealing with Ethical Values and Norms. In this section, participants were asked to rate the \emph{justifiability} of 19 different behaviors or issues with moral connotations. These include topics such as \emph{divorce}, \emph{euthanasia}, \emph{political violence}, \emph{cheating on taxes}, and others. We performed preprocessing by filtering the dataset to retain only the responses to the 19 moral questions (Q177 to Q195) and the country code for each respondent. 


Each response is an integer from 1 to 10. We then mapped the country codes to country names (using the provided codebook) so that each respondent entry includes their country and their answers to the moral questions. Next, we handled missing or non-response values. Entries coded as \(-1\), \(-2\), \(-4\), or \(-5\)~(i.e., \emph{Don’t know}, \emph{No answer}, \emph{Not asked}, and \emph{Missing}) were set to \(0\), so they would not distort later calculations. We then grouped the data by country and averaged the responses for each moral statement. This yields a country-level average moral approval score for each of the 19 issues. Because different countries may use the \(1\)-–\(10\) scale differently (culturally, some may avoid extreme ratings, etc.), and to facilitate comparison with the second dataset, we normalized these country mean scores to a range of \([-1, 1]\), with \(-1\) denoting \emph{never justifiable} and \(+1\) denoting \emph{always justifiable}. 

After these steps, the WVS data provides, for each country and each moral topic, a score between -1 and 1 representing how acceptable that behavior is on average according to that country’s respondents. Higher scores mean the society tends to view the behavior as more acceptable or justifiable, whereas lower scores mean it is seen as less acceptable or not justifiable. We treat these normalized \emph{country-by-topic} scores as the empirical ground truth of moral attitudes.

\begin{figure}[H]
        \centering
        \includegraphics[width=0.4\textwidth]{figures/WVS_boxplot.pdf}
        \vspace{-8pt}
        \caption{\small Spread of responses (country mean scores) across the moral topics for the WVS Wave 7 dataset. }
        \label{fig:wvs_figures_2}
    \end{figure}
\vspace{-10pt}


Figure~\ref{fig:wvs_figures_2} shows the spread of responses across different moral topics and countries. In other words, for each moral topic, how varied are the country scores? Some topics might have very similar scores in every culture (indicating global agreement), while others show a wide range (indicating high cross-cultural controversy). 


\paragraph{PEW Global Attitudes Survey 2013} The PEW collected responses on moral issues from 39 countries, with about 100 respondents per country for the relevant questions\footnote{\url{https://www.pewresearch.org/dataset/spring-2013-survey-data/}}. Unlike WVS, which used a 10-point scale, the PEW survey questions were simpler: for each issue, respondents were asked whether the behavior is \emph{morally acceptable}, \emph{morally unacceptable}, or \emph{not a moral issue}.

From the PEW dataset, we extracted the questions corresponding to those eight moral topics (Q84A to Q84H). We again retained only the country identifier and these responses for our purposes. We coded the responses in a numeric way to be analogous to the WVS scale: for each question, we assigned a value of \(+1\) to \emph{morally acceptable}, \(-1\) to \emph{morally unacceptable}, and \(0\) to \emph{not a moral issue} and all non-responses (including \emph{Depends on situation}, \emph{Refused}, and \emph{Don’t know}). As with WVS, we grouped responses by country, averaged them for each topic, and normalized the averages to \([-1, 1]\). Figure~\ref{fig:pew_figures_2} shows the normalized PEW values across the eight moral questions. The comparison of normalized scores for WVS and PEW by country is also presented in Appendix~\ref{appendix:WVS_PEW_scores_by_country}, Figure~\ref{fig:dist_by_country}.


\begin{figure}[H]
        \centering
        \includegraphics[width=0.45\textwidth]{figures/PEW_boxplot.pdf}
        \vspace{-8pt}
        \caption{Spread of responses across the moral topics and countries for the PEW 2013 dataset.}
        \label{fig:pew_figures_2}
    \end{figure}
\vspace{-12pt}


\subsection{Methodology}

Our evaluation of LLMs involves generating moral judgment scores from the models and comparing them with the two survey data. We first outline the LLMs we selected for testing, then describe how we prompted the models to obtain moral scores for each country and topic. Finally, we detail the three evaluation methods (\emph{correlation analysis}, \emph{cluster alignment analysis}, and \emph{models’ error analysis}) that we applied to quantify the models’ performance.

%\footnote{We will release our code upon acceptance to facilitate reproducibility.}

\paragraph{Model Selection}
We evaluated a broad range of transformer-based, decoder-only language models for their capacity to reflect cross-cultural moral judgments in the WVS and PEW data. Our initial set included the GPT-2 family (\texttt{GPT2-B}, \texttt{GPT2-M}, \texttt{GPT2-L})~\citep{Radford2019} for its coherent text generation at modest scales, as well as \texttt{OPT-125} and \texttt{OPT-350}~\citep{Zhang2022} to examine mid-sized behavior on ethically sensitive content. For multilingual coverage, we tested \texttt{BloomZ}~\citep{Muennighoff2023}, \texttt{Qwen-0.5}, and \texttt{Qwen-72}~\citep{Bai2023}, aiming to see whether broader linguistic training influences moral alignment. We then studied whether larger parameter sizes or instruction tuning could improve consistency by including \texttt{Gemma-9} \citep{Mesnard2024}, \texttt{Llama3-8B}, \texttt{Llama3.3-70I} \citep{Touvron2023a}, and \texttt{Llama2-70} \citep{Touvron2023b}. Additional instruction-tuned models, such as \texttt{Dbrx-inst} \citep{Conover2023a}, \texttt{MPT-30}~\citep{Mosaicml2023}, \texttt{Falcon3-7B}, \texttt{Falcon-40I} \citep{Almazrouei2023}, \texttt{GPT-NeoX20}~\citep{Black2022}, \texttt{T5-L} \citep{raffel2020exploring}, and \texttt{Dolly-12}~\citep{Conover2023b}, covered diverse training setups and parameter scales. We further compared \texttt{Bloom} \citep{LeScao2022} and \texttt{BloomZ}~\citep{Muennighoff2023} to see how instruction-specific methods affect moral responses. Finally, we examined chat-focused or proprietary systems like \texttt{GPT3.5} \citep{brown2020language}, \texttt{GPT4o}~\citep{OpenAI2024b}, and \texttt{GPT4o-mini} \citep{OpenAI2024a} to determine how interactive or closed-source models handle moral content. Importantly, none of these models received additional fine-tuning on moral or cultural data, meaning our findings show their unmodified views on ethical prompts.

\paragraph{Prompt Structuring}
Following the template-based probing framework introduced by~\citet{ramezani2023knowledge} for fine-grained moral-norm inference across cultures, we adapt and extend their basic method to probe a wider range of LLMs.  
To query the models about moral judgments, we designed prompts that mimic the structure of statements about morality in different countries. Our goal was to get the model to complete statements in a way that reveals whether it thinks a behaviour is viewed as moral or immoral in a given culture. We used two main prompt templates for each country–topic pair: 

% \\
% {\small {P1: \emph{In \(\{country\}\), \(\{topic\}\) is \(\{judgment\}\).}}},\\
% {\small {P2: \emph{People in \(\{country\}\) believe \(\{topic\}\) is \(\{judgment\}\).}}}
% \\

%\vspace{0.5em} % Optional: adds vertical space
{\small \textit{P1: In \(\{country\}\), \(\{topic\}\) is \(\{judgment\}\).}}\\
{\small \textit{P2: People in \(\{country\}\) believe \(\{topic\}\) is \(\{judgment\}\).}}

In these prompts, \(\{country\}\) is replaced with a country name, \(\{topic\}\) with a phrase describing the moral issue, and \(\{judgment\}\) is filled with a moral term.


 \paragraph{Moral Judgment Scores} We compute a moral score from the model for each country-topic. Let \(\mathcal{L}\) be a language model. For each moral topic (e.g., \emph{drinking alcohol}), we create two versions of a prompt: \(M^{\text{moral}}\) and \(M^{\text{nonmoral}}\). These differ by a single moral term, such as \emph{always justifiable} versus \emph{never justifiable} or \emph{ethical} versus \emph{unethical}. We then obtain $ log p\bigl(M^{\text{moral}}\bigr)\quad\text{and}\quad logp\bigl(M^{\text{nonmoral}}\bigr),$ which represent \(\mathcal{L}\)'s tendency toward each stance. To reduce the impact of specific word choices, we repeat this process with five moral-adjective pairs \footnote{Always justifiable vs. never justifiable, right vs. wrong, morally good vs. morally bad, ethically right vs. ethically wrong, and ethical vs. unethical} and compute the average difference in log probabilities:
$ 
\Delta \;=\; \log p\bigl(M^\text{moral}\bigr) \;-\; \log p\bigl(M^\text{nonmoral}\bigr).
$
\vspace{4pt}
\\
We apply min--max normalization to \(\Delta\) across all topics and countries, mapping \(\Delta\) into \([-1, +1]\):

\vspace{4pt}
$ 
\Delta_{\text{norm}} \;=\; 2\,\frac{\Delta - \Delta_{\min}}{\Delta_{\max} - \Delta_{\min}} \;-\; 1.
$ 
\vspace{4pt}
\\
The result is a model-based \emph{moral justifiability score} \(s_i \in [-1, +1]\). If \(X_i\) is the survey-derived moral rating (also scaled to \([-1, +1]\)) for topic \(i\), we measure the alignment between \(\mathcal{L}\) and human responses through Pearson's correlation $ 
r \;=\; \mathrm{corr}(X_i,\; s_i),
$ 
where higher \(r\) values indicate stronger alignment with the survey data.

\paragraph{Direct Numerical Rating}
In contrast, for proprietary chat models (e.g., \texttt{GPT4o} and \texttt{GPT4o-mini}), the OpenAI ChatCompletion API does not provide access to token-level log probabilities. Instead, we adopt a direct elicitation approach. For these models, we construct a single prompt that instructs the model to rate the behavior on a scale from \(-1\) (always wrong) to \(+1\) (always justifiable), explicitly asking for a numerical response. Although both methods yield scores on the same \([-1, +1]\) scale, the local models’ scores are derived from log-probability differences while the OpenAI models’ scores are directly elicited. Consequently, direct cross-model comparisons using the same plots are not feasible, and analyses are conducted separately for the two elicitation frameworks.

\paragraph{Cross-Country Correlations and Clustering}
We compare each model’s cross-country correlations on a given topic to the survey-based scores. This correlation analysis shows whether a model senses that certain issues polarize particular cultures. In addition, we represent each country as a vector of moral justifiability scores and apply clustering metrics (e.g., Adjusted Rand Index or Adjusted Mutual Information) to see if a model’s country clusters match survey-derived groupings.

\paragraph{Comparative Prompts}
We explicitly ask the model to compare two countries’ moral judgments on a given topic. We use a direct comparative prompt of the form: 
\\
{{\emph{Regarding the morality of \(\{topic\}\), \(\{country X\}\) and \(\{country Y\}\) are similar.}}}
\\
This tests whether the model recognizes that some pairs of countries hold similar moral views on certain topics. Overall, our pipeline of constructing moral descriptors, calculating log-probability differences, and normalizing them gives a quantitative measure of how well each language model agrees with cross-cultural moral data.

\section{Results}

\subsection{Correlation Analysis}

\paragraph{Pearson correlations} We first evaluated how well each model’s predicted log-prob differences align with the WVS and PEW survey scores by computing Pearson correlations (\(r\)). Table~\ref{tab:AllCorrelationsStars} shows the correlations for all models alongside parameter counts and significance levels.

\begin{table}[ht]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.0}
\vspace{-5pt}
\caption{\small Pearson correlations ($r$) for WVS and PEW. Asterisks denote significance levels: *~$(p<0.05)$, **~$(p<0.01)$, ***~$(p<0.001)$.}
\vspace{-5pt}
\begin{tabular}{llcccc}

\toprule
\textbf{Model} & \textbf{Params} & \multicolumn{2}{c}{\textbf{WVS}} & \multicolumn{2}{c}{\textbf{PEW}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
 & & $r$ &  $p$-value & $r$ &  $p$-value \\
\midrule
GPT2-B         & 117M   & 0.210 & *** & 0.163 & **  \\
GPT2-M         & 355M   & 0.161 & *** & -0.094 &     \\
GPT2-L         & 774M   & 0.007 &     & -0.256 & *** \\
OPT-125        & 125M   & 0.016 &     & 0.127  & *   \\
OPT-350        & 350M   & -0.156 & *** & -0.334 & *** \\
BloomZ         & 560M   & NaN   &     & 0.443  & *** \\
Qwen-0.5       & 500M   & -0.408 & *** & 0.029  &     \\
Qwen-72        & 72B    & -0.078 & *   & -0.060 &     \\
Gemma-9        & 9B     & 0.440  & *** & 0.573  & *** \\
Llama3-8B      & 8B     & 0.161  & *** & 0.151  & **  \\
Llama3.3-70I   & 70B    & 0.036  &     & -0.038 &     \\
Llama2-70      & 70B    & -0.329 & *** & -0.602 & *** \\
Falcon3-7B     & 7B     & -0.312 & *** & -0.415 & *** \\
Falcon-40I     & 40B    & 0.385  & *** & 0.671  & *** \\
GPT-NeoX20     & 20B    & -0.078 & *   & 0.001  &     \\
Dolly-12       & 12B    & -0.247 & *** & 0.010  &     \\
Bloom          & 176B   & -0.048 &     & N/A    &     \\
GPT3.5         & --     & 0.543  & *** & 0.566  & *** \\
GPT4o          & --     & 0.504  & *** & 0.618  & *** \\
GPT4o-mini     & --     & 0.472  & *** & 0.678  & *** \\
\bottomrule
\end{tabular}

\label{tab:AllCorrelationsStars}
\end{table}
\vspace{-5pt}

Models such as \texttt{GPT4o} and \texttt{GPT4o-mini} achieve positive correlations on both WVS and PEW, while others (e.g., \texttt{Qwen-0.5}, \texttt{Llama2-70}) yield negative correlations. Medium-scale instruction-tuned models (e.g., \texttt{Gemma-9}) also show moderate-to-strong alignment, indicating that training approaches and parameter size both influence agreement with survey data.

% \subsection{Country-Level Correlations}
\paragraph{Country-Level Correlations} Next, we computed per-country correlations to see how models fare in different regional contexts. Let \(\mathbf{m}_i\) be the vector of a model’s predicted moral scores for country \(i\) across all topics, and let \(\mathbf{s}_i\) be the corresponding vector of survey-based scores. We compute $r_i = \mathrm{corr}\bigl(\mathbf{m}_i,\;\mathbf{s}_i\bigr)$ for each country \(i\). Figure~\ref{fig:country_heatmaps} shows heatmaps for WVS and PEW datasets, where each row is a model and each column is a country.


\begin{figure}[H]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=9cm,height=20cm,keepaspectratio]{figures/country_heatmap_WVS.pdf}
        \caption{\small WVS}
        \label{fig:country_heatmap_wvs}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=9cm,height=5cm,keepaspectratio]{figures/country_heatmap_PEW.pdf}
        \caption{\small PEW}
        \label{fig:country_heatmap_pew}
    \end{subfigure}
    \vspace{-15pt}
    \caption{\small Per-country correlations, with each cell showing $r$ for a model/country. Red implies higher positive correlation, blue implies negative correlation.}
    \label{fig:country_heatmaps}
\end{figure}

\vspace{-10pt}
In Figure~\ref{fig:country_heatmap_wvs}, models like \texttt{Gemma-9} have strong positive correlations (red squares) with local moral views across many countries, In contrast, some large-scale Llama variants exhibit negative or near-zero correlations (blue or pale squares), indicating disagreements with respondents on specific moral issues. In Figure~\ref{fig:country_heatmap_pew}, no model consistently performs well across all countries. For instance, \texttt{Falcon-40I} has strong support in parts of the Middle East, while others show areas of divergence with surveyed populations. This highlights each model's unique strengths and weaknesses in understanding cross-cultural diversity.

% \subsection{Pairwise Model Correlation and Clustering}
\paragraph{Pairwise Models' Correlations}We then examined the relationships between models by correlating their log-probability difference vectors across all country--topic pairs. For any two models \(X\) and \(Y\), let \(\mathbf{x}\) and \(\mathbf{y}\) denote their respective log-prob difference scores. We compute $\rho_{X,Y} \;=\; \mathrm{corr} \bigl(\mathbf{x},\;\mathbf{y}\bigr)$, thereby producing a \textit{pairwise correlation} matrix among all models.  Figure~\ref{fig:pairwise} shows pairwise correlations for WVS and PEW datasets. Red indicates strong similarity, while blue indicates divergence. 

Figure~\ref{fig:pairwise_wvs} shows that \textbf{GPT2} variants (\texttt{GPT2-B}, \texttt{GPT2-M}, \texttt{GPT2-L}) cluster together, indicating consistent log-probability differences within the same family. In contrast, \texttt{Qwen-0.5} and \texttt{Qwen-72} exhibit weak or negative correlations with instruction-tuned models like \texttt{Falcon-40I} and \texttt{Gemma-9}, suggesting a different approach to morally charged prompts. Similarly, \texttt{BloomZ} aligns more closely with some \texttt{Llama} variants than with \texttt{Dolly-12} or \texttt{GPT-NeoX20}, reflecting differences in training methods. Figure~\ref{fig:pairwise_pew} further reveals moderate to high correlations among related models, with \texttt{GPT3.5} and \texttt{GPT4o} showing strong alignment, while models like \texttt{Llama2-70} and \texttt{Llama3.3-70I} may diverge from older ones like \texttt{GPT2-B}. These findings highlight that instruction tuning and scale produce distinct moral stance patterns, guiding model selection for tasks requiring consistent or diverse moral reasoning and helping identify outlier models with unique stances.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/model_pairwise_correlation_WVS.pdf}
        \caption{WVS}
        \label{fig:pairwise_wvs}
    \end{subfigure}
    \hspace{10pt}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/model_pairwise_correlation_PEW.pdf}
        \caption{PEW}
        \label{fig:pairwise_pew}
    \end{subfigure}
    \vspace{-5pt}
    \caption{\small Pairwise correlation heatmaps of log-prob differences for (a) WVS and (b) PEW.}
    \label{fig:pairwise}
\end{figure}

\vspace{-12pt}



\subsection{Cluster Alignment}
We created hierarchical clustering trees using the pairwise correlations to further analyze how models interrelate in their moral stance predictions.  we treat the distance between any two models \(X\) and \(Y\) as \(d(X,Y) = 1 - \rho_{X,Y}\), where \(\rho_{X,Y}\) is the Pearson correlation of their log-prob differences over all \((\text{country}, \text{topic})\) pairs. A bottom-up agglomerative clustering algorithm then merges the most similar models (lowest distances) at each step, resulting in a dendrogram as shown in Figure \ref{fig:hier}.
% Figures~\ref{fig:hier_wvs_new} and~\ref{fig:hier_pew_new} for WVS and PEW respectively.

In Figure~\ref{fig:hier_wvs_new}, models like \texttt{GPT2-Large} and \texttt{GPT2} are closely grouped, with \texttt{GPT2-Medium} merging slightly higher. A second cluster includes \texttt{Bloom}, \texttt{OPT-125}, and \texttt{Llama3-8B}, showing some shared correlation. Meanwhile, \texttt{Qwen-0.5}, \texttt{Qwen-72}, and \texttt{dolly-v2-12b} form another moderate distance group, while large-scale or instruction-tuned models (e.g., \texttt{GPT3.5-turbo}, \texttt{GPT4o}, \texttt{Falcon-40I}) merge only at the top, suggesting limited similarity in their log-probability difference vectors. Figure~\ref{fig:hier_pew_new} shows a similar structure, with some clusters differing based on the models' responses to the morally focused PEW prompts. Notably, \texttt{GPT2} and \texttt{Gemma-9} cluster at low linkage heights, indicating strong similarities in their probability assignments for morally charged statements. Another cluster includes \texttt{Llama2-70}, \texttt{Falcon3-7B}, and \texttt{GPT-NeoX20}, which may reflect shared training data or architectural features leading to comparable moral stances.

\begin{figure}[H]
    \centering
    % First plot
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hier_dendrogram_WVS.pdf}
        \caption{WVS}
        \label{fig:hier_wvs_new}
    \end{subfigure}%
    \hspace{-5pt}
    % Second plot
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hier_dendrogram_PEW.pdf}
        \caption{PEW}
        \label{fig:hier_pew_new}
    \end{subfigure}
    \vspace{-8pt}
    \caption{\small Hierarchical clustering dendrogram}
    \label{fig:hier}
\end{figure}
\vspace{-5pt}


\subsection{Models' Error}

\paragraph{Absolute Error} To assess each model's deviation from human survey responses, we calculated the absolute difference for each country-topic pair as follows:
\\
$\left|\text{survey\_score} \;-\; \text{model\_prediction}\right|$
\\
Figure~\ref{fig:abs_error_dist} shows these distributions for WVS (~\ref{fig:abs_error_dist_wvs}) and PEW (~\ref{fig:abs_error_dist_pew}), aggregated over all models.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/abs_error_dist_WVS.pdf}
        \caption{}
        \label{fig:abs_error_dist_wvs}
    \end{subfigure}%
    \hspace{-5pt}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/abs_error_dist_PEW.pdf}
        \caption{}
        \label{fig:abs_error_dist_pew}
    \end{subfigure}
    \vspace{-8pt}
    \caption{\small Absolute error distributions across all models for (a) WVS and (b) PEW. Many errors cluster between 0.2 and 0.6, but some exceed 1.0.}
    \label{fig:abs_error_dist}
\end{figure}
\vspace{-8pt}
In the case of WVS (see Figure~\ref{fig:abs_error_dist_wvs}), many predictions fall within an error range of about \(0.2\) to \(0.6\), indicating that model outputs are often close to the average moral ratings provided by respondents. However, there is a significant tail extending beyond \(1.0\), suggesting that for controversial or culturally sensitive topics, model predictions can diverge greatly from real human attitudes. A similar pattern is seen with PEW (see Figure~\ref{fig:abs_error_dist_pew}), where maximum errors rarely exceed \(3.0\). While most country-topic pairs cluster around errors of \(0.2\) to \(1.0\), a notable number exceed \(1.5\) or \(2.0\), highlighting systematic misalignments in specific ethical domains that may vary widely across cultures or lack adequate representation in the training data.


\paragraph{Mean Absolute Error} While correlation captures how well each model’s normalized outputs align with survey responses, we also examine the Mean Absolute Error (MAE) per (\textit{model}, \textit{topic}) pair. This highlights which moral topics each model finds “harder” (higher error) or “easier” (lower error). Figure~\ref{fig:heatmap} displays a heatmap across models (columns) and topics (rows) with darker cells indicating higher error, and Tables~\ref{tab:EasiestTopics} and \ref{tab:HardestTopics} show the ten easiest and hardest topics, respectively, based on average error. 

In Figure~\ref{fig:heatmap}, topics like \emph{political violence}, \emph{suicide}, and \emph{stealing property} result in high errors for multiple models, while issues such as \emph{drinking alcohol}, \emph{using contraceptives}, and \emph{divorce} are generally easier for systems to manage.

In Table~\ref{tab:EasiestTopics}, the topic \emph{using contraceptives} has the highest average error, recorded at 0.51, while the topic \emph{death penalty} has a lower average error of 0.36. A low standard deviation indicates consistent ease across different models, whereas a high standard deviation suggests that only some models find the topic easy to address. In contrast, Table~\ref{tab:HardestTopics} highlights that \emph{political violence} leads the list with an average error of 0.95. This is followed by \emph{suicide}, \emph{stealing property}, and \emph{accepting a bribe while on duty}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{figures/mean_abs_error_heatmap.pdf}
\vspace{-15pt}
\caption{\small Heatmap of mean absolute errors by topic (rows) and model (columns).}
\label{fig:heatmap}
\end{figure}
\vspace{-15pt}
\begin{table}[ht]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.0}
\caption{\small Ten easiest topics (lowest mean absolute error).}
\vspace{-5pt}
\begin{tabular}{lcc}
\toprule
\textbf{Topic} & \textbf{Avg.\ Error} & \textbf{Std.\ Dev.}\\
\midrule
using contraceptives & 0.5111 & 0.2109 \\
gambling & 0.4911 & 0.1632 \\
drinking alcohol & 0.4815 & 0.1115 \\
parents beating children & 0.4622 & 0.2617 \\
getting a divorce & 0.4311 & 0.0824 \\
having casual sex & 0.4075 & 0.2079 \\
divorce & 0.3913 & 0.0723 \\
claiming govt.\ benefits not entitled & 0.3862 & 0.1991 \\
euthanasia & 0.3838 & 0.0792 \\
death penalty & 0.3633 & 0.1472 \\
\bottomrule
\end{tabular}
\label{tab:EasiestTopics}
\end{table}
\vspace{-8pt}
\begin{table}[ht]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.0}
\caption{\small Ten hardest topics (highest mean absolute error).}
\vspace{-5pt}
\begin{tabular}{lcc}
\toprule
\textbf{Topic} & \textbf{Avg.\ Error} & \textbf{Std.\ Dev.}\\
\midrule
political violence & 0.9546 & 0.3650 \\
suicide & 0.9229 & 0.2486 \\
stealing property & 0.8393 & 0.3416 \\
someone accepting a bribe & 0.7998 & 0.3738 \\
for a man to beat his wife & 0.7819 & 0.2878 \\
cheating on taxes & 0.7170 & 0.3617 \\
violence against other people & 0.7091 & 0.3323 \\
terrorism (political/ideological) & 0.6919 & 0.2806 \\
homosexuality & 0.6056 & 0.1665 \\
abortion & 0.5985 & 0.3104 \\
\bottomrule
\end{tabular}
\label{tab:HardestTopics}
\end{table}



\section{Discussion and Conclusion}
\label{sec:discussion_Conclusion}

Our findings show that language models vary considerably in how well they replicate cross-cultural moral judgments, as captured in the WVS and PEW surveys. Larger or instruction-tuned models, such as \texttt{Falcon-40I}, \texttt{Gemma-9}, and \texttt{GPT4o}, frequently demonstrate higher correlations with aggregated human survey responses. In contrast, some models, including \texttt{Qwen-0.5} and \texttt{Llama2-70}, yield systematically negative correlations, suggesting that scale alone does not guarantee alignment with moral attitudes if the underlying training data or methodology is insufficiently diverse or biased.

In addition, topic-level analysis reveals that certain issues (e.g., political violence, terrorism, or wife-beating) consistently produce higher mean errors across different architectures. These discrepancies suggest that moral questions involving violence or extreme social norms may pose particular challenges for current language models, especially when training data do not include nuanced representations of such topics. Even models that perform relatively well on broad measures sometimes fail on region-specific or contentious issues. This trend aligns with evidence that LLMs handle clear-cut moral scenarios well but often display uncertainty or divergence on morally ambiguous dilemmas~\citep{scherrer2023evaluating}. Per-country heatmaps similarly highlight that no single model excels in all areas: while a model may align with opinions in Western nations, it can deviate markedly in communities whose moral or cultural practices are underrepresented in its training corpora.

Despite these limitations, instruction-tuned and larger models show promise in better reflecting overall moral consensus in many cases. This suggests that scaling models and using tailored training, where instructions or datasets capture diverse viewpoints, can improve moral judgment alignment. However, performance still varies, highlighting the need to analyze results in detail (e.g., by topic or country) rather than relying on a single global metric. From an applied perspective, these insights can guide the development of more culturally responsive AI systems, for example, informing content moderation policies or chatbot designs that respect regional norms.

\subsection{Limitations}
\label{sec:limitations}

Although our methodology offers insights into cross-cultural moral alignment in language models, it has several limitations that should be acknowledged. First, the WVS and PEW data capture broad national averages and may not fully reflect within-country heterogeneity, especially in regions with significant cultural or linguistic diversity. Second, our log-probability difference calculation relies on short prompt templates, which might not elicit the full context required for more complex moral issues. Third, the models we evaluated differ in size, instruction tuning, and training data composition, making it challenging to isolate the effect of each factor.

A further limitation arises from the necessity of employing distinct evaluation strategies. For local models, we have access to token-level log probabilities, enabling us to compute log-probability differences as a proxy for moral judgment. However, for OpenAI’s proprietary chat models, we rely on directly elicited numerical scores because the API does not expose internal log probabilities. This divergence means that the resulting moral scores are derived from different underlying mechanisms, precluding a direct, unified comparison of model outputs in our visualizations. Future work might seek alternative methods to bridge this gap or develop metrics that are comparable across elicitation approaches.


\section{Conclusion}
In conclusion, our analysis of moral stance alignment across WVS and PEW data underscores both the progress and the continuing gaps in LLMs' performance. Models with substantial parameter counts and instruction-tuned frameworks frequently achieve moderate-to-high correlations with surveyed human judgments, suggesting an ability to capture broad moral viewpoints. However, sizable deviations persist on sensitive topics and in particular cultural contexts, indicating that no current model entirely overcomes biases or data deficiencies. Thus, while larger or more specialized training procedures can improve a model’s capacity to reflect human moral attitudes, they do not guarantee universal alignment. Future work must address these persistent shortcomings through expanded training corpora, targeted bias mitigation, and refined evaluation protocols that account for cultural and topic-level nuances.

\section*{Ethical considerations}
\label{sec:impact}

Using language models in real-world applications has important ethical implications and risks. Even though these models can approximate broad moral opinions, they may misrepresent local or minority viewpoints if their training data is not diverse enough. This misrepresentation can lead to biases or stereotypes, especially on sensitive topics like domestic violence, religious norms, or political extremism. If a model’s output is mistakenly viewed as a true reflection of public opinion, automated decisions could unfairly target or exclude certain groups, worsening existing inequalities. Moreover, significant misalignment on controversial topics can undermine public trust if model predictions seem harmful or insensitive. To reduce such risks, it is vital to include diverse voices and expert feedback when building and testing these models. Adding regular evaluations on moral or cultural issues, transparent reports of known biases, and human review for high-stakes decisions, can help ensure ethical and responsible deployment. As language models evolve, balancing technical progress with careful oversight will be essential for maintaining fairness and trust in automated systems.

\section*{Funding}
This research received no external funding.           

\section*{Disclosure statement}
The authors declare no conflict of interest.


\section*{Data and code availability}
The full source code, experiment scripts, and processed datasets are openly available on GitHub.\footnote{\url{https://github.com/mohammadi-hadi/cultural-moral-judgments-llms}}

%A static snapshot corresponding to the version used in this article is preserved on Zenodo (DOI: 10.5281/zenodo.XXXXXXX).

\section*{Author Contributions}

H.M. and A.B. conceptualized the research. H.M., E.P., Y.M., and A.B. developed the methodology. H.M., E.P., and Y.M. contributed to software implementation, while H.M. and A.B. handled validation. E.P., Y.M., and H.M. composed the original draft, and H.M. and A.B. oversaw review and editing.


\section*{Acknowledgments}
%\vspace{-5pt}
We appreciate the maintainers of WVS and PEW data for enabling large-scale cross-cultural analysis. We also thank Dr. Anastasia Giachanou for her valuable comments and feedback and Tina Shahedi for her editorial contributions to this paper. We also thank SURF for providing the computational facilities.
%The authors used OpenAI's ChatGPT-4o exclusively for grammar and style checks. They have reviewed the content thoroughly and take full responsibility for the final manuscript.

%\clearpage



%\bibliographystyle{acl_natbib}
\bibliography{references}

%\bibliography{custom}


\appendix
%\newpage


\section{Topic Codes for WVS and PEW}
\label{appendix:topic_codes}

\begin{table}[ht]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.1}
\caption{Mapping of Topic Codes to the Dataset (WVS or PEW) and their corresponding moral questions.}
\label{tab:topic_mapping}
\vspace{5pt}
\begin{tabular}{p{0.4cm}p{0.5cm}p{5.7cm}}
\toprule
\textbf{Topic Code} & \textbf{Dataset} & \textbf{Moral Question} \\ 
\midrule
Q177 & WVS & Claiming government benefits to which you are not entitled \\
Q178 & WVS & Avoiding a fare on public transport \\
Q179 & WVS & Stealing property \\
Q180 & WVS & Cheating on taxes \\
Q181 & WVS & Someone accepting a bribe in the course of their duties \\
Q182 & WVS & Homosexuality \\
Q183 & WVS & Prostitution \\
Q184 & WVS & Abortion \\
Q185 & WVS & Divorce \\
Q186 & WVS & Sex before marriage \\
Q187 & WVS & Suicide \\
Q188 & WVS & Euthanasia \\
Q189 & WVS & For a man to beat his wife \\
Q190 & WVS & Parents beating children \\
Q191 & WVS & Violence against other people \\
Q192 & WVS & Terrorism as a political, ideological or religious mean \\
Q193 & WVS & Having casual sex \\
Q194 & WVS & Political violence \\
Q195 & WVS & Death penalty \\
Q84A & PEW & Using contraceptives \\
Q84B & PEW & Getting a divorce \\
Q84C & PEW & Having an abortion \\
Q84D & PEW & Homosexuality \\
Q84E & PEW & Drinking alcohol \\
Q84F & PEW & Married people having an affair \\
Q84G & PEW & Gambling \\
Q84H & PEW & Sex between unmarried adults \\
\bottomrule
\end{tabular}
\end{table}

\section{WVS \& PEW scores by country}
\label{appendix:WVS_PEW_scores_by_country}
Figure~\ref{fig:dist_by_country} compares normalized WVS (orange) and PEW (gold) scores by country. Each box shows the interquartile range, with medians as horizontal lines and diamonds marking outliers. The broader spread in the WVS data for many countries suggests higher variance in moral acceptance. Some countries, such as the United States or Czech Republic, show very wide ranges, from near \(-1\) (\emph{never justifiable}) to close to \(+1\) (\emph{always justifiable}). Others, often in the Middle East or South Asia, have more negative medians, reflecting stricter cultural norms on certain issues.

\begin{figure*}[ht]
\centering
\begin{minipage}{\textwidth} % Adjust width as needed
    \centering
    \includegraphics[width=\linewidth]{figures/appendix/score_distribution_country.pdf}
    \vspace{-15pt}
    \caption{\small Distribution of normalized WVS (orange) and PEW (gold) survey scores by country.}
    \label{fig:dist_by_country}
\end{minipage}
\end{figure*}


\section{Individual Figures by Model \& Dataset}

In each scatter plot, the horizontal axis \(\text{survey\_score}\) corresponds to \texttt{WVS} in Figure \ref{fig:scatter_wvs} and \texttt{PEW} ratings in Figure \ref{fig:scatter_pew}. Meanwhile, the vertical axis \(\text{log\_prob\_diff}\) shows the difference between the log-probability the model assigns to a \emph{morally justifiable} statement vs.\ a \emph{morally unjustifiable} statement. A positive slope suggests that higher survey acceptance correlates with higher log-prob differences in the same direction, meaning better alignment. Conversely, negative slopes may show systematic misalignment on that dimension.


\begin{figure*}[ht]
\centering
\renewcommand{\arraystretch}{1} % Adjust vertical spacing between rows
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 1 (3 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/01-gpt2_WVS_scatter.pdf}
\caption*{\scriptsize GPT2-B}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/02-gpt2-medium_WVS_scatter.pdf}
\caption*{\scriptsize GPT2-M}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/03-gpt2-large_WVS_scatter.pdf}
\caption*{\scriptsize GPT2-L}
\end{minipage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 2 (3 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/04-facebook_opt-125m_WVS_scatter.pdf}
\caption*{\scriptsize OPT-125}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/05-facebook_opt-350m_WVS_scatter.pdf}
\caption*{\scriptsize OPT-350}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/06-bigscience_bloomz-560m_WVS_scatter.pdf}
\caption*{\scriptsize BloomZ}
\end{minipage}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 3 (3 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip1em
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/07-Qwen_Qwen2-0.5B_WVS_scatter.pdf}
\caption*{\scriptsize Qwen-0.5}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/08-Qwen_Qwen2.5-72B_WVS_scatter.pdf}
\caption*{\scriptsize Qwen-72}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/09-meta-llama_Meta-Llama-3-8B_WVS_scatter.pdf}
\caption*{\scriptsize Llama3-8B}
\end{minipage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 4 (3 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/010-meta-llama_Llama-3.3-70B-Instruct_WVS_scatter.pdf}
\caption*{\scriptsize Llama3.3-70I}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/011-meta-llama_Llama-2-70b-hf_WVS_scatter.pdf}
\caption*{\scriptsize Llama3.3-70I}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/012-tiiuae_Falcon3-7B-Base_WVS_scatter.pdf}
\caption*{\scriptsize Falcon3-7B}
\end{minipage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 5 (3 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip1em
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/013-tiiuae_falcon-40b-instruct_WVS_scatter.pdf}
\caption*{\scriptsize Falcon-40I}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/014-EleutherAI_gpt-neox-20b_WVS_scatter.pdf}
\caption*{\scriptsize GPT-NeoX20}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/015-databricks_dolly-v2-12b_WVS_scatter.pdf}
\caption*{\scriptsize Dolly-12}
\end{minipage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 6 (2 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/016-bigscience_bloom_WVS_scatter.pdf}
\caption*{\scriptsize Bloom}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/017-llama70b_WVS_scatter.pdf}
\caption*{\scriptsize Llama2-70}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
%\includegraphics[width=\linewidth]{}
\caption*{\scriptsize }
\end{minipage}
\caption{\small Scatter plots for WVS dataset}
\label{fig:scatter_wvs}
\end{figure*}



%PEW
\begin{figure*}[ht]
\centering

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 1 (3 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip1em
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/11-gpt2_PEW_scatter.pdf}
\caption*{\scriptsize GPT2-B}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/12-gpt2-medium_PEW_scatter.pdf}
\caption*{\scriptsize GPT2-M}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/13-gpt2-large_PEW_scatter.pdf}
\caption*{\scriptsize GPT2-L}
\end{minipage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 2 (3 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/14-facebook_opt-125m_PEW_scatter.pdf}
\caption*{\scriptsize OPT-125}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/15-facebook_opt-350m_PEW_scatter.pdf}
\caption*{\scriptsize OPT-350}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/16-bigscience_bloomz-560m_PEW_scatter.pdf}
\caption*{\scriptsize BloomZ}
\end{minipage}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 3 (3 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip1em
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/17-Qwen_Qwen2-0.5B_PEW_scatter.pdf}
\caption*{\scriptsize Qwen-0.5}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/18-Qwen_Qwen2.5-72B_PEW_scatter.pdf}
\caption*{\scriptsize Qwen-72}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/19-meta-llama_Meta-Llama-3-8B_PEW_scatter.pdf}
\caption*{\scriptsize Llama3-8B}
\end{minipage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 4 (3 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/110-meta-llama_Llama-3.3-70B-Instruct_PEW_scatter.pdf}
\caption*{\scriptsize Llama3.3-70I}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/111-meta-llama_Llama-2-70b-hf_PEW_scatter.pdf}
\caption*{\scriptsize Llama3.3-70I}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/112-tiiuae_Falcon3-7B-Base_PEW_scatter.pdf}
\caption*{\scriptsize Falcon3-7B}
\end{minipage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 5 (6 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip1em
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/113-tiiuae_falcon-40b-instruct_PEW_scatter.pdf}
\caption*{\scriptsize Falcon-40I}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/114-EleutherAI_gpt-neox-20b_PEW_scatter.pdf}
\caption*{\scriptsize GPT-NeoX20}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/115-databricks_dolly-v2-12b_PEW_scatter.pdf}
\caption*{\scriptsize Dolly-12}
\end{minipage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Row 6 (2 plots):
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/116-google_gemma-2-9b-it_PEW_scatter.pdf}
\caption*{\scriptsize Bloom}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/plots_scatter_new/117-meta-llama_Llama-2-70b-hf_PEW_PEW_scatter.pdf}
\caption*{\scriptsize Llama2-70}
\end{minipage}
\hfill
\begin{minipage}[t]{0.15\linewidth}
\centering
%\includegraphics[width=\linewidth]{}
\caption*{\scriptsize }
\end{minipage}

\caption{\small Scatter plots for PEW dataset}
\label{fig:scatter_pew}
\end{figure*}


\end{document}
