# Model Configuration File for Moral Alignment Pipeline
# Updated December 2024 with latest models
# Complete specifications for 40+ state-of-the-art models

# ==============================================================================
# Small Pre-trained Models (Baseline)
# ==============================================================================

gpt2_family:
  - name: "gpt2"
    model_id: "gpt2"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "117M"
    year: "2019"
    
  - name: "gpt2-medium"
    model_id: "gpt2-medium"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "345M"
    year: "2019"
    
  - name: "gpt2-large"
    model_id: "gpt2-large"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "774M"
    year: "2019"
    
  - name: "gpt2-xl"
    model_id: "gpt2-xl"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "1.5B"
    year: "2019"

# ==============================================================================
# OPT Models (Meta)
# ==============================================================================

opt_family:
  - name: "opt-125m"
    model_id: "facebook/opt-125m"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "125M"
    
  - name: "opt-350m"
    model_id: "facebook/opt-350m"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "350M"
    
  - name: "opt-1.3b"
    model_id: "facebook/opt-1.3b"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "1.3B"
    
  - name: "opt-2.7b"
    model_id: "facebook/opt-2.7b"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "2.7B"

# ==============================================================================
# Multilingual Models
# ==============================================================================

multilingual:
  - name: "bloomz-560m"
    model_id: "bigscience/bloomz-560m"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "560M"
    
  - name: "bloomz-1b7"
    model_id: "bigscience/bloomz-1b7"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "1.7B"
    
  - name: "bloom-176b"
    model_id: "bigscience/bloom"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true  # Large model, use 8-bit
    parameters: "176B"

# ==============================================================================
# Qwen Models (Alibaba)
# ==============================================================================

qwen_family:
  - name: "qwen-0.5b"
    model_id: "Qwen/Qwen2-0.5B"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "0.5B"
    
  - name: "qwen-1.8b"
    model_id: "Qwen/Qwen-1_8B"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "1.8B"
    
  - name: "qwen-72b"
    model_id: "Qwen/Qwen2.5-72B"
    engine: null
    is_chat: false
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "72B"

# ==============================================================================
# Instruction-Tuned Models (Latest 2024)
# ==============================================================================

instruction_tuned:
  - name: "llama-3.3-70b-instruct"
    model_id: "meta-llama/Llama-3.3-70B-Instruct"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "70B"
    year: "2024"
    
  - name: "llama-3.2-90b-instruct"
    model_id: "meta-llama/Llama-3.2-90B-Instruct"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "90B"
    year: "2024"
    
  - name: "llama-3.2-11b-instruct"
    model_id: "meta-llama/Llama-3.2-11B-Instruct"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "11B"
    year: "2024"
    
  - name: "llama-3.2-3b-instruct"
    model_id: "meta-llama/Llama-3.2-3B-Instruct"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "3B"
    year: "2024"
    
  - name: "llama-3.2-1b-instruct"
    model_id: "meta-llama/Llama-3.2-1B-Instruct"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "1B"
    year: "2024"
    
  - name: "gemma-2-27b-it"
    model_id: "google/gemma-2-27b-it"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "27B"
    year: "2024"
    
  - name: "gemma-2-9b-it"
    model_id: "google/gemma-2-9b-it"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "9B"
    year: "2024"
    
  - name: "gemma-2-2b-it"
    model_id: "google/gemma-2-2b-it"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "2B"
    year: "2024"
    
  - name: "mistral-large-2"
    model_id: "mistralai/Mistral-Large-2"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "123B"
    year: "2024"
    
  - name: "mixtral-8x22b-instruct"
    model_id: "mistralai/Mixtral-8x22B-Instruct-v0.1"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "141B"
    year: "2024"
    
  - name: "command-r-plus"
    model_id: "CohereForAI/c4ai-command-r-plus"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "104B"
    year: "2024"

# ==============================================================================
# Falcon Models
# ==============================================================================

falcon_family:
  - name: "falcon-7b"
    model_id: "tiiuae/Falcon3-7B-Base"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    parameters: "7B"
    
  - name: "falcon-40b-instruct"
    model_id: "tiiuae/falcon-40b-instruct"
    engine: null
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "40B"

# ==============================================================================
# Other Models
# ==============================================================================

other_models:
  - name: "gpt-neox-20b"
    model_id: "EleutherAI/gpt-neox-20b"
    engine: null
    is_chat: false
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "20B"
    
  - name: "dolly-v2-12b"
    model_id: "databricks/dolly-v2-12b"
    engine: null
    is_chat: true
    max_tokens: 256
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: true
    parameters: "12B"

# ==============================================================================
# API-Based Models (Latest 2024)
# ==============================================================================

api_models:
  # OpenAI Models (2024)
  - name: "gpt-4o"
    model_id: "gpt-4o"
    engine: "gpt-4o-2024-11-20"
    is_chat: true
    max_tokens: 1024
    temperature: 0.7
    top_p: 0.95
    api_provider: "openai"
    year: "2024"
    
  - name: "gpt-4o-mini"
    model_id: "gpt-4o-mini"
    engine: "gpt-4o-mini-2024-07-18"
    is_chat: true
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    api_provider: "openai"
    year: "2024"
    
  - name: "gpt-4-turbo"
    model_id: "gpt-4-turbo"
    engine: "gpt-4-turbo-2024-04-09"
    is_chat: true
    max_tokens: 1024
    temperature: 0.7
    top_p: 0.95
    api_provider: "openai"
    year: "2024"
    
  - name: "o1-preview"
    model_id: "o1-preview"
    engine: "o1-preview-2024-09-12"
    is_chat: true
    max_tokens: 32768
    temperature: 1.0
    top_p: 1.0
    api_provider: "openai"
    year: "2024"
    note: "Reasoning model with extended thinking"
    
  - name: "o1-mini"
    model_id: "o1-mini"
    engine: "o1-mini-2024-09-12"
    is_chat: true
    max_tokens: 65536
    temperature: 1.0
    top_p: 1.0
    api_provider: "openai"
    year: "2024"
    note: "Smaller reasoning model"
    
  # Google Gemini Models (2024)
  - name: "gemini-1.5-pro"
    model_id: "gemini-1.5-pro"
    engine: "gemini-1.5-pro-002"
    is_chat: true
    max_tokens: 8192
    temperature: 0.7
    top_p: 0.95
    api_provider: "google"
    year: "2024"
    context_window: "2M tokens"
    
  - name: "gemini-1.5-flash"
    model_id: "gemini-1.5-flash"
    engine: "gemini-1.5-flash-002"
    is_chat: true
    max_tokens: 8192
    temperature: 0.7
    top_p: 0.95
    api_provider: "google"
    year: "2024"
    context_window: "1M tokens"
    
  - name: "gemini-1.5-flash-8b"
    model_id: "gemini-1.5-flash-8b"
    engine: "gemini-1.5-flash-8b-exp-0827"
    is_chat: true
    max_tokens: 8192
    temperature: 0.7
    top_p: 0.95
    api_provider: "google"
    year: "2024"
    note: "Experimental smaller version"
    
  - name: "gemini-2.0-flash-exp"
    model_id: "gemini-2.0-flash-exp"
    engine: "gemini-2.0-flash-exp"
    is_chat: true
    max_tokens: 8192
    temperature: 0.7
    top_p: 0.95
    api_provider: "google"
    year: "2024"
    note: "Experimental next-gen model"
    
  # Anthropic Claude Models (2024)
  - name: "claude-3.5-sonnet"
    model_id: "claude-3.5-sonnet"
    engine: "claude-3-5-sonnet-20241022"
    is_chat: true
    max_tokens: 8192
    temperature: 0.7
    top_p: 0.95
    api_provider: "anthropic"
    year: "2024"
    context_window: "200K tokens"
    
  - name: "claude-3.5-haiku"
    model_id: "claude-3.5-haiku"
    engine: "claude-3-5-haiku-20241022"
    is_chat: true
    max_tokens: 8192
    temperature: 0.7
    top_p: 0.95
    api_provider: "anthropic"
    year: "2024"
    note: "Faster, more affordable"
    
  - name: "claude-3-opus"
    model_id: "claude-3-opus"
    engine: "claude-3-opus-20240229"
    is_chat: true
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.95
    api_provider: "anthropic"
    year: "2024"
    context_window: "200K tokens"
    
  # Mistral API Models (2024)
  - name: "mistral-large-latest"
    model_id: "mistral-large-latest"
    engine: "mistral-large-2411"
    is_chat: true
    max_tokens: 1024
    temperature: 0.7
    top_p: 0.95
    api_provider: "mistral"
    year: "2024"
    parameters: "123B"
    
  - name: "mistral-small-latest"
    model_id: "mistral-small-latest"
    engine: "mistral-small-2411"
    is_chat: true
    max_tokens: 1024
    temperature: 0.7
    top_p: 0.95
    api_provider: "mistral"
    year: "2024"
    parameters: "22B"
    
  # Cohere API Models (2024)
  - name: "command-r-plus-api"
    model_id: "command-r-plus"
    engine: "command-r-plus-08-2024"
    is_chat: true
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.95
    api_provider: "cohere"
    year: "2024"
    parameters: "104B"
    
  - name: "command-r-api"
    model_id: "command-r"
    engine: "command-r-08-2024"
    is_chat: true
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.95
    api_provider: "cohere"
    year: "2024"
    parameters: "35B"

# ==============================================================================
# Experimental Settings
# ==============================================================================

experiment_config:
  # Chain-of-thought settings
  cot_params:
    num_samples: 5
    temperature_override: 0.7
    top_p_override: 0.95
    
  # Peer critique settings
  critique_params:
    max_words: 60
    temperature: 0.3
    
  # Self-consistency settings
  embedding_model: "all-MiniLM-L6-v2"
  
  # Resource management
  batch_size: 16
  max_parallel_api_calls: 5
  checkpoint_frequency: 100
  
  # Output settings
  save_traces: true
  save_intermediate: true
  generate_visualizations: true

# ==============================================================================
# Deployment Profiles
# ==============================================================================

deployment_profiles:
  # For testing with limited resources
  minimal:
    models: ["gpt2", "llama-3.2-1b-instruct", "gemma-2-2b-it"]
    sample_size: 20
    batch_size: 4
    
  # Standard evaluation
  standard:
    models: ["gpt2", "llama-3.2-3b-instruct", "llama-3.2-11b-instruct", "gemma-2-9b-it", "mistral-small-latest"]
    sample_size: null  # Full dataset
    batch_size: 8
    
  # Latest 2024 models
  cutting_edge_2024:
    models: [
      "llama-3.3-70b-instruct", 
      "llama-3.2-90b-instruct",
      "gemma-2-27b-it",
      "mistral-large-2",
      "gpt-4o",
      "gemini-1.5-pro",
      "claude-3.5-sonnet",
      "o1-preview"
    ]
    sample_size: null
    batch_size: 4
    note: "Latest state-of-the-art models from 2024"
    
  # Lightweight models for quick testing
  lightweight:
    models: [
      "llama-3.2-1b-instruct",
      "llama-3.2-3b-instruct", 
      "gemma-2-2b-it",
      "gpt-4o-mini",
      "gemini-1.5-flash-8b"
    ]
    sample_size: 50
    batch_size: 8
    
  # Full paper replication
  full:
    models: "all"
    sample_size: null
    batch_size: 16
    
  # API models only (2024)
  api_only_2024:
    models: [
      "gpt-4o", 
      "gpt-4o-mini",
      "o1-preview",
      "gemini-1.5-pro",
      "gemini-1.5-flash",
      "claude-3.5-sonnet",
      "claude-3.5-haiku",
      "mistral-large-latest"
    ]
    sample_size: null
    batch_size: 1
    
  # Mixed local and API
  hybrid:
    models: [
      "llama-3.2-11b-instruct",
      "gemma-2-9b-it",
      "gpt-4o-mini",
      "gemini-1.5-flash",
      "claude-3.5-haiku"
    ]
    sample_size: null
    batch_size: 4