# Moral Alignment Pipeline - Complete Implementation

## ğŸ“š Overview

This is a complete implementation of the research paper **"Exploring Cultural Variations in Moral Judgments with Large Language Models"**. The pipeline evaluates 30+ language models on cross-cultural moral judgments using World Values Survey (WVS) and PEW Research data.

### Key Features:
- âœ… Support for 30+ models (local and API-based)
- âœ… Dual elicitation: log-probability and direct scoring
- âœ… Chain-of-thought reasoning with 3-step protocol
- âœ… Reciprocal peer critique system
- âœ… Self-consistency calculation
- âœ… Comprehensive visualizations
- âœ… Easy deployment on any server

## ğŸš€ Quick Start

### Option 1: Jupyter Notebook (Recommended for Exploration)

1. **Open the notebook:**
   ```bash
   jupyter notebook moral_alignment_complete.ipynb
   ```

2. **Set API keys (if using API models):**
   ```python
   import os
   os.environ['OPENAI_API_KEY'] = 'your-key-here'
   os.environ['GEMINI_API_KEY'] = 'your-key-here'
   ```

3. **Run all cells** or step through interactively

### Option 2: Command Line Script (Recommended for Batch Processing)

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Run with minimal profile (quick test):**
   ```bash
   python run_experiments.py --profile minimal --sample-size 20
   ```

3. **Run full evaluation:**
   ```bash
   python run_experiments.py --profile full
   ```

## ğŸ“ Project Structure

```
Project06/
â”œâ”€â”€ moral_alignment_complete.ipynb  # Main Jupyter notebook (all-in-one)
â”œâ”€â”€ run_experiments.py              # Command-line runner
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ models_config.yaml              # Model configurations
â”œâ”€â”€ sample_data/                    # Survey data files
â”‚   â”œâ”€â”€ WVS_Moral.csv              # World Values Survey data
â”‚   â”œâ”€â”€ Country_Codes_Names.csv    # Country mappings
â”‚   â””â”€â”€ Pew Research *.sav         # PEW survey data
â””â”€â”€ outputs/                        # Generated results (auto-created)
    â”œâ”€â”€ *_lp_scores.csv            # Log-probability scores
    â”œâ”€â”€ *_dir_scores.csv           # Direct scores
    â”œâ”€â”€ traces/                    # Reasoning traces
    â”œâ”€â”€ figures/                   # Visualizations
    â””â”€â”€ all_metrics.csv            # Summary metrics
```

## ğŸ”§ Installation

### Requirements:
- Python 3.8+
- CUDA 11.8+ (for GPU support)
- 24GB+ GPU memory (for large models)

### Step 1: Clone and Install
```bash
# Clone the repository
git clone <repository-url>
cd Project06

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Set Environment Variables
```bash
# For API models (optional)
export OPENAI_API_KEY="your-openai-api-key"
export GEMINI_API_KEY="your-gemini-api-key"

# Or create a .env file
echo "OPENAI_API_KEY=your-key" >> .env
echo "GEMINI_API_KEY=your-key" >> .env
```

## ğŸ¯ Usage Examples

### 1. Test with Small Models
```python
# In Jupyter notebook
MODELS_TO_EVALUATE = ['gpt2', 'opt-125m']
SAMPLE_SIZE = 20  # Quick test
```

### 2. Evaluate Specific Models
```bash
python run_experiments.py --models gpt2 gpt2-medium opt-350m --sample-size 50
```

### 3. Run Full Paper Replication
```bash
python run_experiments.py --profile full --output-dir full_results
```

### 4. API Models Only
```bash
python run_experiments.py --profile api_only
```

### 5. Skip API Models (Local Only)
```bash
python run_experiments.py --profile standard --skip-api
```

## ğŸ“Š Available Models

### Small Pre-trained Models:
- GPT-2 (117M, 345M, 774M, 1.5B)
- OPT (125M, 350M, 1.3B, 2.7B)

### Multilingual Models:
- BLOOMZ (560M, 1.7B, 176B)
- Qwen (0.5B, 1.8B, 72B)

### Instruction-Tuned Models:
- Gemma-2-9B-IT
- Llama-3.3-70B-Instruct
- Llama-3-8B-Instruct

### API Models:
- GPT-3.5-turbo
- GPT-4o, GPT-4o-mini
- Gemini-1.5-Pro, Gemini-1.5-Flash

## ğŸŒ Deployment Options

### Google Colab
```python
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Upload notebook and data
# Run cells with Colab runtime
```

### AWS/Cloud Server
```bash
# SSH to server
ssh user@server

# Clone repository
git clone <repo-url>

# Install dependencies
pip install -r requirements.txt

# Run experiments
nohup python run_experiments.py --profile full &
```

### Docker Container
```dockerfile
FROM pytorch/pytorch:2.0.0-cuda11.8-cudnn8-runtime
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
CMD ["python", "run_experiments.py", "--profile", "standard"]
```

## ğŸ“ˆ Outputs and Visualizations

### Generated Files:
- **Scores**: `outputs/*_lp_scores.csv`, `outputs/*_dir_scores.csv`
- **Traces**: `outputs/traces/*_traces.jsonl`
- **Metrics**: `outputs/all_metrics.csv`
- **Figures**: `outputs/figures/*.png`

### Visualizations:
1. **Correlation Comparison**: WVS vs PEW alignment
2. **Self-Consistency**: Model reasoning stability
3. **Country Heatmaps**: Geographic patterns
4. **Topic Difficulty**: Challenging moral topics
5. **Model Clustering**: Behavioral similarity

## âš™ï¸ Configuration

### Modify `models_config.yaml`:
```yaml
deployment_profiles:
  custom:
    models: ["gpt2", "gemma-2-9b-it", "gpt-4o-mini"]
    sample_size: 100
    batch_size: 8
```

### Use Custom Profile:
```bash
python run_experiments.py --profile custom
```

## ğŸ› Troubleshooting

### Out of Memory:
- Use 8-bit quantization: `load_in_8bit: true`
- Reduce batch size
- Use smaller models first

### API Rate Limits:
- Add delays between API calls
- Use `--skip-api` for local models only

### Missing Data Files:
- Ensure `sample_data/` contains WVS and PEW files
- Check file paths in configuration

## ğŸ“ Citation

If you use this implementation, please cite:
```bibtex
@article{mohammadi2024cultural,
  title={Exploring Cultural Variations in Moral Judgments with Large Language Models},
  author={Mohammadi, H. and others},
  year={2024}
}
```

## ğŸ“§ Support

For issues or questions:
- Create an issue on GitHub
- Email: h.mohammadi@uu.nl

## ğŸ“„ License

MIT License - See LICENSE file for details

---

**Note**: This implementation provides a complete, reproducible pipeline that can be easily deployed on any server with GPU access. All code is contained in the Jupyter notebook for maximum portability.