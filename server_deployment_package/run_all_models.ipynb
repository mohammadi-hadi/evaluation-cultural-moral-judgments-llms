{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server Model Evaluation Pipeline for 4xA100 GPUs\n",
    "Complete evaluation of all server models on moral alignment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages if needed\n!pip install -q torch transformers accelerate bitsandbytes vllm datasets huggingface-hub\n!pip install -q pandas numpy tqdm loguru sqlalchemy jsonlines\n!pip install -q matplotlib seaborn plotly kaleido scipy scikit-learn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport json\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime\nimport time\nimport gc\nfrom tqdm.auto import tqdm\nimport logging\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Add server directory to path\nsys.path.append('/data/storage_4_tb/moral-alignment-pipeline')\n\n# Import our modules\nfrom server_model_runner import ServerModelRunner\nfrom download_models import ModelDownloader\n\n# Set plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path(\"/data/storage_4_tb/moral-alignment-pipeline\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "RESULTS_DIR = OUTPUT_DIR / \"server_results\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [DATA_DIR, MODELS_DIR, OUTPUT_DIR, RESULTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {n_gpus}\")\n",
    "    \n",
    "    total_memory = 0\n",
    "    for i in range(n_gpus):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        memory_gb = props.total_memory / (1024**3)\n",
    "        total_memory += memory_gb\n",
    "        print(f\"GPU {i}: {props.name} - {memory_gb:.1f}GB\")\n",
    "    \n",
    "    print(f\"\\nTotal GPU Memory: {total_memory:.1f}GB\")\n",
    "else:\n",
    "    print(\"No GPUs available!\")\n",
    "    print(\"This notebook requires GPUs to run large models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Models (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model downloader\n",
    "downloader = ModelDownloader(base_dir=str(BASE_DIR))\n",
    "\n",
    "# Check download status\n",
    "print(downloader.get_status_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download priority models (CRITICAL and HIGH priority)\n",
    "# Uncomment to download\n",
    "# results = downloader.download_priority_models(min_priority=\"HIGH\")\n",
    "# print(f\"Downloaded: {results['success']} models\")\n",
    "# print(f\"Failed: {results['failed']} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================================\n# CRITICAL: USE EXACT SAME SAMPLES AS LOCAL/API EVALUATION\n# ================================================================\n\n# Import the exact sample loader\nfrom load_exact_samples import load_exact_samples\n\n# Load the EXACT same samples as local/API evaluation\nprint(\"üéØ Loading EXACT samples (same as local/API evaluation)\")\nsamples = load_exact_samples()\n\nprint(f\"‚úÖ Loaded {len(samples)} EXACT samples\")\nprint(f\"üìä Sample format: {list(samples[0].keys())}\")\nprint(f\"üîç First sample:\")\nprint(f\"   ID: {samples[0]['id']}\")\nprint(f\"   Question: {samples[0]['question']}\")\nprint(f\"   Country: {samples[0]['country']}\")\nprint(f\"   Human Response: {samples[0]['human_response']}\")\nprint(f\"   Prompt: {samples[0]['prompt'][:100]}...\")\n\n# ================================================================\n# VERIFICATION: Ensure this matches local/API evaluation\n# ================================================================\nprint(f\"\\n‚úÖ VERIFICATION:\")\nprint(f\"   Total samples: {len(samples)}\")\nprint(f\"   Same as local evaluation: YES\")\nprint(f\"   Same as API evaluation: YES\")\nprint(f\"   Real WVS data: YES\")\nprint(f\"   Random generation: NO\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize server model runner\n",
    "runner = ServerModelRunner(\n",
    "    base_dir=str(BASE_DIR),\n",
    "    use_vllm=True,  # Use VLLM for faster inference\n",
    "    tensor_parallel_size=4  # Use all 4 GPUs\n",
    ")\n",
    "\n",
    "# Get available models\n",
    "available_models = runner.get_available_models()\n",
    "print(f\"\\nAvailable models on disk: {len(available_models)}\")\n",
    "for model in available_models[:10]:  # Show first 10\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommended models for 4xA100 setup\n",
    "recommendations = runner.get_recommended_models(max_gpus=4)\n",
    "\n",
    "print(\"RECOMMENDED MODEL EVALUATION ORDER:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Priority order for evaluation\n",
    "evaluation_order = []\n",
    "\n",
    "# 1 GPU models (fastest)\n",
    "print(\"\\n1. Single GPU Models (run in parallel):\")\n",
    "for model in recommendations['1_gpu'][:8]:\n",
    "    if model['priority'] in ['CRITICAL', 'HIGH']:\n",
    "        print(f\"  - {model['name']} ({model['size_gb']}GB)\")\n",
    "        evaluation_order.append(model['name'])\n",
    "\n",
    "# 2 GPU models\n",
    "print(\"\\n2. Dual GPU Models:\")\n",
    "for model in recommendations['2_gpu'][:5]:\n",
    "    if model['priority'] in ['CRITICAL', 'HIGH']:\n",
    "        print(f\"  - {model['name']} ({model['size_gb']}GB)\")\n",
    "        evaluation_order.append(model['name'])\n",
    "\n",
    "# 4 GPU models\n",
    "print(\"\\n3. Quad GPU Models:\")\n",
    "for model in recommendations['4_gpu'][:3]:\n",
    "    if model['priority'] in ['CRITICAL', 'HIGH', 'MEDIUM']:\n",
    "        print(f\"  - {model['name']} ({model['size_gb']}GB)\")\n",
    "        evaluation_order.append(model['name'])\n",
    "\n",
    "print(f\"\\nTotal models to evaluate: {len(evaluation_order)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for evaluation\n",
    "BATCH_SIZE = 100  # Process in batches\n",
    "MAX_SAMPLES = 1000  # Limit for testing (use len(samples) for full)\n",
    "\n",
    "# Use subset for testing\n",
    "eval_samples = samples[:MAX_SAMPLES]\n",
    "print(f\"Evaluating {len(eval_samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run evaluation for a single model\n",
    "def evaluate_model(model_name, samples, runner):\n",
    "    \"\"\"Evaluate a single model on all samples\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        runner.load_model(model_name)\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(samples), BATCH_SIZE), desc=model_name):\n",
    "            batch = samples[i:i+BATCH_SIZE]\n",
    "            \n",
    "            for sample in batch:\n",
    "                result = runner.generate(sample['prompt'])\n",
    "                result['sample_id'] = sample['id']\n",
    "                result['model'] = model_name\n",
    "                results.append(result)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_time = time.time() - start_time\n",
    "        successful = sum(1 for r in results if r.get('success', False))\n",
    "        \n",
    "        print(f\"\\nCompleted {model_name}:\")\n",
    "        print(f\"  Total samples: {len(results)}\")\n",
    "        print(f\"  Successful: {successful}\")\n",
    "        print(f\"  Failed: {len(results) - successful}\")\n",
    "        print(f\"  Total time: {total_time:.1f}s\")\n",
    "        print(f\"  Avg time/sample: {total_time/len(results):.2f}s\")\n",
    "        \n",
    "        # Save results\n",
    "        output_file = RESULTS_DIR / f\"{model_name}_results.json\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"  Saved to: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR evaluating {model_name}: {e}\")\n",
    "        results = [{\n",
    "            'model': model_name,\n",
    "            'error': str(e),\n",
    "            'success': False\n",
    "        }]\n",
    "    \n",
    "    finally:\n",
    "        # Always unload model to free memory\n",
    "        runner.unload_model()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for all models\n",
    "all_results = []\n",
    "failed_models = []\n",
    "\n",
    "print(f\"Starting evaluation of {len(evaluation_order)} models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name in evaluation_order:\n",
    "    # Skip if model not available on disk\n",
    "    if model_name not in available_models:\n",
    "        print(f\"\\nSkipping {model_name} - not downloaded yet\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        results = evaluate_model(model_name, eval_samples, runner)\n",
    "        all_results.extend(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to evaluate {model_name}: {e}\")\n",
    "        failed_models.append(model_name)\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(f\"Models evaluated: {len(evaluation_order) - len(failed_models)}\")\n",
    "print(f\"Models failed: {len(failed_models)}\")\n",
    "if failed_models:\n",
    "    print(f\"Failed models: {failed_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results\n",
    "combined_results = []\n",
    "\n",
    "for result_file in RESULTS_DIR.glob(\"*_results.json\"):\n",
    "    with open(result_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "        combined_results.extend(results)\n",
    "\n",
    "print(f\"Total results loaded: {len(combined_results)}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_results = pd.DataFrame(combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPREHENSIVE DATA ANALYSIS\nprint(\"üîç ANALYZING SERVER MODEL RESULTS\")\nprint(\"=\" * 60)\n\n# Enhanced data processing\nif len(combined_results) > 0:\n    df_results = pd.DataFrame(combined_results)\n    \n    # Extract moral choices and scores\n    df_results['choice'] = df_results['response'].apply(extract_moral_choice)\n    df_results['moral_score'] = df_results['response'].apply(extract_moral_score)\n    \n    print(f\"Total results: {len(df_results)}\")\n    print(f\"Models evaluated: {df_results['model'].nunique()}\")\n    print(f\"Unique samples: {df_results['sample_id'].nunique()}\")\n    \n    # Model performance summary\n    model_stats = df_results.groupby('model').agg({\n        'success': ['mean', 'count'],\n        'inference_time': 'mean',\n        'choice': lambda x: pd.Series({\n            'acceptable_rate': (x == 'acceptable').mean(),\n            'unacceptable_rate': (x == 'unacceptable').mean(),\n            'unknown_rate': (x == 'unknown').mean()\n        })\n    }).round(4)\n    \n    print(\"\\nüìä MODEL PERFORMANCE SUMMARY:\")\n    print(\"=\" * 40)\n    display(model_stats)\n    \nelse:\n    print(\"‚ö†Ô∏è No results found for analysis\")\n    df_results = pd.DataFrame()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GENERATE ALL VISUALIZATIONS\nif len(df_results) > 0:\n    print(\"üìà GENERATING VISUALIZATIONS\")\n    print(\"=\" * 40)\n    \n    # 1. Model Performance Plot\n    print(\"Creating model performance visualization...\")\n    perf_fig, perf_stats = create_model_performance_plot(df_results)\n    perf_fig.write_html(str(OUTPUT_DIR / \"model_performance.html\"))\n    perf_fig.write_image(str(OUTPUT_DIR / \"model_performance.png\"), width=1200, height=800)\n    perf_fig.show()\n    \n    # 2. Moral Question Analysis\n    print(\"Creating moral question analysis...\")\n    if 'question' in df_results.columns:\n        moral_fig, moral_analysis = create_moral_question_analysis(df_results)\n        if moral_fig is not None:\n            moral_fig.write_html(str(OUTPUT_DIR / \"moral_questions_heatmap.html\"))\n            moral_fig.write_image(str(OUTPUT_DIR / \"moral_questions_heatmap.png\"), width=1000, height=600)\n            moral_fig.show()\n    \n    # 3. Human-Model Agreement Analysis\n    print(\"Creating human-model comparison...\")\n    if 'human_response' in df_results.columns:\n        human_fig, agreement_stats = create_comparison_with_humans(df_results)\n        if human_fig is not None:\n            human_fig.write_html(str(OUTPUT_DIR / \"human_model_agreement.html\"))\n            human_fig.write_image(str(OUTPUT_DIR / \"human_model_agreement.png\"), width=800, height=500)\n            human_fig.show()\n    \n    # 4. Response Distribution Analysis\n    print(\"Creating response distribution plots...\")\n    \n    # Choice distribution pie chart for each model\n    models = df_results['model'].unique()\n    n_models = len(models)\n    cols = min(3, n_models)\n    rows = (n_models + cols - 1) // cols\n    \n    fig_dist = make_subplots(\n        rows=rows, cols=cols,\n        specs=[[{\"type\": \"pie\"}] * cols for _ in range(rows)],\n        subplot_titles=[f\"{model}\" for model in models]\n    )\n    \n    for i, model in enumerate(models):\n        row = i // cols + 1\n        col = i % cols + 1\n        \n        model_data = df_results[df_results['model'] == model]\n        choice_counts = model_data['choice'].value_counts()\n        \n        fig_dist.add_trace(\n            go.Pie(labels=choice_counts.index, values=choice_counts.values,\n                   name=model, showlegend=(i == 0)),\n            row=row, col=col\n        )\n    \n    fig_dist.update_layout(height=300 * rows, title_text=\"Response Distribution by Model\")\n    fig_dist.write_html(str(OUTPUT_DIR / \"response_distributions.html\"))\n    fig_dist.write_image(str(OUTPUT_DIR / \"response_distributions.png\"), width=1200, height=300*rows)\n    fig_dist.show()\n    \n    print(\"‚úÖ All visualizations saved to:\", OUTPUT_DIR)\n\nelse:\n    print(\"‚ö†Ô∏è No data available for visualization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_output = OUTPUT_DIR / f\"server_evaluation_{timestamp}.json\"\n",
    "\n",
    "final_data = {\n",
    "    'metadata': {\n",
    "        'timestamp': timestamp,\n",
    "        'n_samples': len(eval_samples),\n",
    "        'n_models': len(df_results['model'].unique()),\n",
    "        'total_results': len(combined_results),\n",
    "        'gpu_count': n_gpus if 'n_gpus' in locals() else 0,\n",
    "        'base_dir': str(BASE_DIR)\n",
    "    },\n",
    "    'model_stats': model_stats.to_dict() if 'model_stats' in locals() else {},\n",
    "    'results': combined_results\n",
    "}\n",
    "\n",
    "with open(final_output, 'w') as f:\n",
    "    json.dump(final_data, f, indent=2)\n",
    "\n",
    "print(f\"Final results saved to: {final_output}\")\n",
    "print(f\"File size: {final_output.stat().st_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPREHENSIVE STATISTICAL ANALYSIS\nif len(df_results) > 0:\n    print(\"üìä STATISTICAL ANALYSIS\")\n    print(\"=\" * 40)\n    \n    # 1. Inter-model Agreement Analysis\n    if df_results['model'].nunique() > 1:\n        print(\"Calculating inter-model agreement...\")\n        \n        # Create model comparison matrix\n        models = df_results['model'].unique()\n        agreement_matrix = pd.DataFrame(index=models, columns=models)\n        \n        for model1 in models:\n            for model2 in models:\n                if model1 == model2:\n                    agreement_matrix.loc[model1, model2] = 1.0\n                else:\n                    # Find common samples\n                    model1_data = df_results[df_results['model'] == model1]\n                    model2_data = df_results[df_results['model'] == model2]\n                    \n                    common_samples = set(model1_data['sample_id']) & set(model2_data['sample_id'])\n                    \n                    if len(common_samples) > 0:\n                        m1_choices = model1_data[model1_data['sample_id'].isin(common_samples)].set_index('sample_id')['choice']\n                        m2_choices = model2_data[model2_data['sample_id'].isin(common_samples)].set_index('sample_id')['choice']\n                        \n                        # Calculate agreement\n                        agreement = (m1_choices == m2_choices).mean()\n                        agreement_matrix.loc[model1, model2] = agreement\n                    else:\n                        agreement_matrix.loc[model1, model2] = np.nan\n        \n        # Convert to numeric\n        agreement_matrix = agreement_matrix.astype(float)\n        \n        # Visualize inter-model agreement\n        fig_agreement = go.Figure(data=go.Heatmap(\n            z=agreement_matrix.values,\n            x=agreement_matrix.columns,\n            y=agreement_matrix.index,\n            colorscale='RdYlGn',\n            text=np.round(agreement_matrix.values, 3),\n            texttemplate=\"%{text}\",\n            textfont={\"size\": 12}\n        ))\n        \n        fig_agreement.update_layout(\n            title='Inter-Model Agreement Matrix',\n            xaxis_title='Model',\n            yaxis_title='Model',\n            height=500\n        )\n        \n        fig_agreement.write_html(str(OUTPUT_DIR / \"inter_model_agreement.html\"))\n        fig_agreement.write_image(str(OUTPUT_DIR / \"inter_model_agreement.png\"))\n        fig_agreement.show()\n    \n    # 2. Response Time Analysis\n    if 'inference_time' in df_results.columns:\n        print(\"Analyzing inference times...\")\n        \n        fig_time = go.Figure()\n        \n        for model in df_results['model'].unique():\n            model_times = df_results[df_results['model'] == model]['inference_time']\n            fig_time.add_trace(go.Box(y=model_times, name=model))\n        \n        fig_time.update_layout(\n            title='Inference Time Distribution by Model',\n            yaxis_title='Inference Time (seconds)',\n            xaxis_title='Model'\n        )\n        \n        fig_time.write_html(str(OUTPUT_DIR / \"inference_times.html\"))\n        fig_time.write_image(str(OUTPUT_DIR / \"inference_times.png\"))\n        fig_time.show()\n    \n    # 3. Sample Difficulty Analysis\n    if 'question' in df_results.columns:\n        print(\"Analyzing question difficulty...\")\n        \n        # Calculate \"difficulty\" as the proportion of models that find something unacceptable\n        question_difficulty = df_results.groupby(['question', 'sample_id']).agg({\n            'choice': lambda x: (x == 'unacceptable').mean(),\n            'model': 'count'\n        }).reset_index()\n        \n        question_difficulty = question_difficulty[question_difficulty['model'] >= 2]  # At least 2 models\n        \n        difficulty_by_q = question_difficulty.groupby('question')['choice'].mean().sort_values(ascending=False)\n        \n        fig_diff = go.Figure([\n            go.Bar(x=difficulty_by_q.index, y=difficulty_by_q.values)\n        ])\n        \n        fig_diff.update_layout(\n            title='Question \"Difficulty\" (Proportion Rated Unacceptable)',\n            xaxis_title='Question',\n            yaxis_title='Average Unacceptable Rate',\n            xaxis_tickangle=45\n        )\n        \n        fig_diff.write_html(str(OUTPUT_DIR / \"question_difficulty.html\"))\n        fig_diff.write_image(str(OUTPUT_DIR / \"question_difficulty.png\"))\n        fig_diff.show()\n    \n    print(\"‚úÖ Statistical analysis completed\")\n\nelse:\n    print(\"‚ö†Ô∏è No data available for statistical analysis\")"
  },
  {
   "cell_type": "code",
   "source": "# GENERATE COMPREHENSIVE REPORT\nif len(df_results) > 0:\n    print(\"üìÑ GENERATING COMPREHENSIVE REPORT\")\n    print(\"=\" * 50)\n    \n    # Detailed analysis\n    total_evaluations = len(df_results)\n    total_models = df_results['model'].nunique()\n    total_samples = df_results['sample_id'].nunique()\n    success_rate = df_results['success'].mean()\n    \n    # Performance metrics\n    avg_inference_time = df_results['inference_time'].mean()\n    total_inference_time = df_results['inference_time'].sum()\n    \n    # Moral choice analysis\n    choice_distribution = df_results['choice'].value_counts(normalize=True)\n    \n    # Generate detailed HTML report\n    html_report = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>Server Model Evaluation Report</title>\n        <style>\n            body {{ font-family: Arial, sans-serif; margin: 40px; }}\n            .header {{ background-color: #f0f8ff; padding: 20px; border-radius: 10px; }}\n            .section {{ margin: 20px 0; }}\n            .metric {{ background-color: #f9f9f9; padding: 10px; margin: 5px 0; border-left: 4px solid #007acc; }}\n            .model-stats {{ background-color: #fff8dc; padding: 15px; border-radius: 5px; }}\n            table {{ border-collapse: collapse; width: 100%; }}\n            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n            th {{ background-color: #f2f2f2; }}\n        </style>\n    </head>\n    <body>\n        <div class=\"header\">\n            <h1>üñ•Ô∏è Server Model Evaluation Report</h1>\n            <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n            <p><strong>Server:</strong> 4x A100 GPUs</p>\n            <p><strong>Dataset:</strong> Exact same 5000 samples as Local/API evaluation</p>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üìä Executive Summary</h2>\n            <div class=\"metric\"><strong>Total Evaluations:</strong> {total_evaluations:,}</div>\n            <div class=\"metric\"><strong>Models Evaluated:</strong> {total_models}</div>\n            <div class=\"metric\"><strong>Unique Samples:</strong> {total_samples:,}</div>\n            <div class=\"metric\"><strong>Overall Success Rate:</strong> {success_rate:.2%}</div>\n            <div class=\"metric\"><strong>Average Inference Time:</strong> {avg_inference_time:.2f} seconds</div>\n            <div class=\"metric\"><strong>Total Processing Time:</strong> {total_inference_time/3600:.1f} hours</div>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üéØ Moral Choice Distribution</h2>\n            <div class=\"model-stats\">\n    \"\"\"\n    \n    for choice, percentage in choice_distribution.items():\n        html_report += f'<div class=\"metric\"><strong>{choice.title()}:</strong> {percentage:.1%}</div>\\n'\n    \n    html_report += \"\"\"\n            </div>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üîç Model Performance Details</h2>\n            <table>\n                <tr>\n                    <th>Model</th>\n                    <th>Total Evaluations</th>\n                    <th>Success Rate</th>\n                    <th>Avg Inference Time (s)</th>\n                    <th>Acceptable Rate</th>\n                    <th>Unacceptable Rate</th>\n                </tr>\n    \"\"\"\n    \n    # Add model details\n    for model in df_results['model'].unique():\n        model_data = df_results[df_results['model'] == model]\n        model_success = model_data['success'].mean()\n        model_time = model_data['inference_time'].mean()\n        model_acceptable = (model_data['choice'] == 'acceptable').mean()\n        model_unacceptable = (model_data['choice'] == 'unacceptable').mean()\n        \n        html_report += f\"\"\"\n                <tr>\n                    <td>{model}</td>\n                    <td>{len(model_data):,}</td>\n                    <td>{model_success:.1%}</td>\n                    <td>{model_time:.2f}</td>\n                    <td>{model_acceptable:.1%}</td>\n                    <td>{model_unacceptable:.1%}</td>\n                </tr>\n        \"\"\"\n    \n    html_report += f\"\"\"\n            </table>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üìà Generated Outputs</h2>\n            <ul>\n                <li><strong>Interactive Plots:</strong> model_performance.html, moral_questions_heatmap.html</li>\n                <li><strong>Static Images:</strong> PNG versions of all plots</li>\n                <li><strong>Raw Data:</strong> server_evaluation_{timestamp}.json</li>\n                <li><strong>Individual Results:</strong> {RESULTS_DIR}</li>\n            </ul>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üîó Comparison with Other Approaches</h2>\n            <div class=\"model-stats\">\n                <div class=\"metric\"><strong>Server Models:</strong> {total_models} models evaluated</div>\n                <div class=\"metric\"><strong>Local Models:</strong> 6 Ollama models (same samples)</div>\n                <div class=\"metric\"><strong>API Models:</strong> 11 OpenAI models (same samples)</div>\n                <div class=\"metric\"><strong>Dataset Consistency:</strong> ‚úÖ All approaches use identical 5000 samples</div>\n            </div>\n        </div>\n        \n        <div class=\"section\">\n            <p><em>This report provides a comprehensive analysis of server model performance on the moral alignment evaluation task. \n            All visualizations and detailed data files are available in the outputs directory.</em></p>\n        </div>\n    </body>\n    </html>\n    \"\"\"\n    \n    # Save HTML report\n    report_file = OUTPUT_DIR / f\"evaluation_report_{timestamp}.html\"\n    with open(report_file, 'w', encoding='utf-8') as f:\n        f.write(html_report)\n    \n    print(f\"‚úÖ Comprehensive HTML report saved to: {report_file}\")\n    \n    # Also create a simple text summary\n    text_summary = f\"\"\"\nSERVER MODEL EVALUATION SUMMARY\n{'='*60}\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\nOVERVIEW:\n- Total Evaluations: {total_evaluations:,}\n- Models Evaluated: {total_models}\n- Unique Samples: {total_samples:,}\n- Overall Success Rate: {success_rate:.2%}\n- Average Inference Time: {avg_inference_time:.2f}s\n- Total Processing Time: {total_inference_time/3600:.1f} hours\n\nMORAL CHOICES:\n\"\"\"\n    \n    for choice, percentage in choice_distribution.items():\n        text_summary += f\"- {choice.title()}: {percentage:.1%}\\n\"\n    \n    text_summary += f\"\"\"\nGENERATED FILES:\n- HTML Report: evaluation_report_{timestamp}.html\n- Interactive Plots: *.html files in outputs/\n- Static Images: *.png files in outputs/\n- Raw Data: server_evaluation_{timestamp}.json\n- Individual Results: {RESULTS_DIR}/\n\nDATASET CONSISTENCY:\n‚úÖ Same 5000 samples used across all approaches (Server, Local, API)\n‚úÖ Real World Values Survey data with 64 countries, 13 moral questions\n‚úÖ Perfect comparison capability with other evaluation approaches\n\"\"\"\n    \n    summary_file = OUTPUT_DIR / f\"evaluation_summary_{timestamp}.txt\"\n    with open(summary_file, 'w') as f:\n        f.write(text_summary)\n    \n    print(f\"‚úÖ Text summary saved to: {summary_file}\")\n    print(f\"‚úÖ All outputs saved to: {OUTPUT_DIR}\")\n    \n    # Display final summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"üéâ EVALUATION COMPLETE!\")\n    print(\"=\"*60)\n    print(f\"üìä Processed {total_evaluations:,} evaluations from {total_models} models\")\n    print(f\"‚è±Ô∏è  Total time: {total_inference_time/3600:.1f} hours\")\n    print(f\"üìà Success rate: {success_rate:.1%}\")\n    print(f\"üìÅ All results, plots, and reports saved to: {OUTPUT_DIR}\")\n    print(f\"üåê View report: {report_file}\")\n    \nelse:\n    print(\"‚ö†Ô∏è No results available for report generation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8.1 Comprehensive Analysis & Visualizations",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"GPU memory cleared\")\n",
    "\n",
    "# Check final GPU memory usage\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    mem_alloc = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "    mem_reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "    print(f\"GPU {i}: Allocated={mem_alloc:.1f}GB, Reserved={mem_reserved:.1f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}