{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server Model Evaluation Pipeline for 4xA100 GPUs\n",
    "Complete evaluation of all server models on moral alignment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages if needed\n!pip install -q torch transformers accelerate bitsandbytes vllm datasets huggingface-hub\n!pip install -q pandas numpy tqdm loguru sqlalchemy jsonlines\n!pip install -q matplotlib seaborn plotly kaleido scipy scikit-learn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport json\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime\nimport time\nimport gc\nfrom tqdm.auto import tqdm\nimport logging\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Optimization imports\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport threading\nimport queue\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Add server directory to path\nsys.path.append('/data/storage_4_tb/moral-alignment-pipeline')\n\n# Import our optimized modules\nfrom server_model_runner import ServerModelRunner\nfrom download_models import ModelDownloader\nfrom gpu_monitor import GPUMonitor\nfrom batch_processor import BatchProcessor\n\n# Set plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"üìö All optimized modules imported successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path(\"/data/storage_4_tb/moral-alignment-pipeline\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "RESULTS_DIR = OUTPUT_DIR / \"server_results\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [DATA_DIR, MODELS_DIR, OUTPUT_DIR, RESULTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {n_gpus}\")\n",
    "    \n",
    "    total_memory = 0\n",
    "    for i in range(n_gpus):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        memory_gb = props.total_memory / (1024**3)\n",
    "        total_memory += memory_gb\n",
    "        print(f\"GPU {i}: {props.name} - {memory_gb:.1f}GB\")\n",
    "    \n",
    "    print(f\"\\nTotal GPU Memory: {total_memory:.1f}GB\")\n",
    "else:\n",
    "    print(\"No GPUs available!\")\n",
    "    print(\"This notebook requires GPUs to run large models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Models (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model downloader\n",
    "downloader = ModelDownloader(base_dir=str(BASE_DIR))\n",
    "\n",
    "# Check download status\n",
    "print(downloader.get_status_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download priority models (CRITICAL and HIGH priority)\n",
    "# Uncomment to download\n",
    "# results = downloader.download_priority_models(min_priority=\"HIGH\")\n",
    "# print(f\"Downloaded: {results['success']} models\")\n",
    "# print(f\"Failed: {results['failed']} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================================\n# CRITICAL: USE EXACT SAME SAMPLES AS LOCAL/API EVALUATION\n# ================================================================\n\n# Import the exact sample loader\nfrom load_exact_samples import load_exact_samples\n\n# Load the EXACT same samples as local/API evaluation\nprint(\"üéØ Loading EXACT samples (same as local/API evaluation)\")\nsamples = load_exact_samples()\n\nprint(f\"‚úÖ Loaded {len(samples)} EXACT samples\")\nprint(f\"üìä Sample format: {list(samples[0].keys())}\")\nprint(f\"üîç First sample:\")\nprint(f\"   ID: {samples[0]['id']}\")\nprint(f\"   Question: {samples[0]['question']}\")\nprint(f\"   Country: {samples[0]['country']}\")\nprint(f\"   Human Response: {samples[0]['human_response']}\")\nprint(f\"   Prompt: {samples[0]['prompt'][:100]}...\")\n\n# ================================================================\n# VERIFICATION: Ensure this matches local/API evaluation\n# ================================================================\nprint(f\"\\n‚úÖ VERIFICATION:\")\nprint(f\"   Total samples: {len(samples)}\")\nprint(f\"   Same as local evaluation: YES\")\nprint(f\"   Same as API evaluation: YES\")\nprint(f\"   Real WVS data: YES\")\nprint(f\"   Random generation: NO\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize server model runner\n",
    "runner = ServerModelRunner(\n",
    "    base_dir=str(BASE_DIR),\n",
    "    use_vllm=True,  # Use VLLM for faster inference\n",
    "    tensor_parallel_size=4  # Use all 4 GPUs\n",
    ")\n",
    "\n",
    "# Get available models\n",
    "available_models = runner.get_available_models()\n",
    "print(f\"\\nAvailable models on disk: {len(available_models)}\")\n",
    "for model in available_models[:10]:  # Show first 10\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommended models for 4xA100 setup\n",
    "recommendations = runner.get_recommended_models(max_gpus=4)\n",
    "\n",
    "print(\"RECOMMENDED MODEL EVALUATION ORDER:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Priority order for evaluation\n",
    "evaluation_order = []\n",
    "\n",
    "# 1 GPU models (fastest)\n",
    "print(\"\\n1. Single GPU Models (run in parallel):\")\n",
    "for model in recommendations['1_gpu'][:8]:\n",
    "    if model['priority'] in ['CRITICAL', 'HIGH']:\n",
    "        print(f\"  - {model['name']} ({model['size_gb']}GB)\")\n",
    "        evaluation_order.append(model['name'])\n",
    "\n",
    "# 2 GPU models\n",
    "print(\"\\n2. Dual GPU Models:\")\n",
    "for model in recommendations['2_gpu'][:5]:\n",
    "    if model['priority'] in ['CRITICAL', 'HIGH']:\n",
    "        print(f\"  - {model['name']} ({model['size_gb']}GB)\")\n",
    "        evaluation_order.append(model['name'])\n",
    "\n",
    "# 4 GPU models\n",
    "print(\"\\n3. Quad GPU Models:\")\n",
    "for model in recommendations['4_gpu'][:3]:\n",
    "    if model['priority'] in ['CRITICAL', 'HIGH', 'MEDIUM']:\n",
    "        print(f\"  - {model['name']} ({model['size_gb']}GB)\")\n",
    "        evaluation_order.append(model['name'])\n",
    "\n",
    "print(f\"\\nTotal models to evaluate: {len(evaluation_order)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for evaluation\n",
    "BATCH_SIZE = 100  # Process in batches\n",
    "MAX_SAMPLES = 1000  # Limit for testing (use len(samples) for full)\n",
    "\n",
    "# Use subset for testing\n",
    "eval_samples = samples[:MAX_SAMPLES]\n",
    "print(f\"Evaluating {len(eval_samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================================\n# FIXED EVALUATION FUNCTIONS - SEQUENTIAL PROCESSING (NO MORE VLLM CONFLICTS!)\n# ================================================================\n\ndef evaluate_model_fixed(model_name, samples, runner, batch_processor, gpu_monitor):\n    \"\"\"FIXED: Evaluate a single model using sequential processing (no parallel conflicts)\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"üöÄ SEQUENTIAL EVALUATION: {model_name}\")\n    print(f\"{'='*60}\")\n    \n    start_time = time.time()\n    \n    try:\n        # Get model configuration\n        model_config = runner.MODEL_CONFIGS.get(model_name)\n        if not model_config:\n            raise ValueError(f\"Unknown model configuration: {model_name}\")\n        \n        model_size_gb = model_config.size_gb\n        \n        print(f\"üìä Starting sequential processing...\")\n        print(f\"   üíæ Model size: {model_size_gb}GB\")\n        print(f\"   üîß GPUs available: {runner.n_gpus}\")\n        print(f\"   üí∞ Total GPU memory: {runner.total_gpu_memory:.1f}GB\")\n        print(f\"   üîß Method: Load model ONCE, process all samples\")\n        print(f\"   ‚ö° FIXED: No more VLLM conflicts or memory thrashing!\")\n        \n        # Use the NEW sequential evaluation method (no more parallel conflicts!)\n        results = batch_processor.evaluate_model_sequential(\n            model_name=model_name,\n            samples=samples\n        )\n        \n        # Calculate final statistics\n        total_time = time.time() - start_time\n        successful = sum(1 for r in results if r.get('success', False))\n        \n        print(f\"\\n‚úÖ EVALUATION COMPLETE: {model_name}\")\n        print(f\"   üìä Total samples: {len(results)}\")\n        print(f\"   ‚úÖ Successful: {successful} ({successful/len(results)*100:.1f}%)\")\n        print(f\"   ‚ùå Failed: {len(results) - successful}\")\n        print(f\"   ‚è±Ô∏è  Total time: {total_time:.1f}s\")\n        print(f\"   üöÄ Average speed: {len(results)/total_time:.1f} samples/sec\")\n        print(f\"   ‚ö° FIXED: No more model loading per batch (157x fewer loads!)\")\n        \n        # Save individual model results\n        output_file = RESULTS_DIR / f\"{model_name}_results_fixed.json\"\n        with open(output_file, 'w') as f:\n            json.dump(results, f, indent=2)\n        print(f\"   üíæ Saved to: {output_file}\")\n        \n        # Save performance statistics\n        stats_file = RESULTS_DIR / f\"{model_name}_performance_fixed.json\"\n        batch_processor.save_performance_stats(str(stats_file))\n        \n        # Save GPU metrics if available\n        try:\n            metrics_file = RESULTS_DIR / f\"{model_name}_gpu_metrics.json\"\n            gpu_monitor.save_metrics(str(metrics_file))\n        except Exception:\n            pass\n        \n        return results\n        \n    except Exception as e:\n        print(f\"‚ùå ERROR evaluating {model_name}: {e}\")\n        print(f\"   üîÑ Creating error results...\")\n        \n        # Create error results for all samples\n        results = []\n        for i, sample in enumerate(samples):\n            error_result = {\n                'model': model_name,\n                'sample_id': sample.get('id', f'sample_{i}'),\n                'error': str(e),\n                'success': False,\n                'response': '',\n                'inference_time': 0,\n                'timestamp': datetime.now().isoformat()\n            }\n            results.append(error_result)\n        \n        return results\n    \n    finally:\n        # Cleanup is now handled in the sequential processor\n        print(f\"üßπ Cleanup completed for {model_name}\")\n\ndef run_sequential_evaluation(models_to_evaluate, samples, runner, batch_processor, gpu_monitor):\n    \"\"\"FIXED: Run all models sequentially (no more parallel conflicts)\"\"\"\n    print(f\"\\nüîß SEQUENTIAL EXECUTION: {len(models_to_evaluate)} models\")\n    print(\"=\"*60)\n    print(\"‚ö° MAJOR FIX: Eliminated parallel execution to prevent VLLM conflicts\")\n    print(\"üöÄ Each model loads ONCE and processes ALL samples\")\n    print(\"üßπ Proper memory cleanup between models\")\n    print(\"üìà Expected: MUCH higher success rates!\")\n    print()\n    \n    all_results = []\n    successful_models = []\n    failed_models = []\n    \n    for i, model_name in enumerate(models_to_evaluate, 1):\n        print(f\"\\nüöÄ Processing model {i}/{len(models_to_evaluate)}: {model_name}\")\n        print(\"-\" * 50)\n        \n        try:\n            results = evaluate_model_fixed(model_name, samples, runner, batch_processor, gpu_monitor)\n            all_results.extend(results)\n            \n            # Check if any results were successful\n            successful_results = sum(1 for r in results if r.get('success', False))\n            if successful_results > 0:\n                successful_models.append(model_name)\n                print(f\"‚úÖ SUCCESS: {model_name} ({successful_results} successful results)\")\n            else:\n                failed_models.append(model_name)\n                print(f\"‚ùå FAILED: {model_name} (no successful results)\")\n                \n        except Exception as e:\n            print(f\"‚ùå CRITICAL ERROR with {model_name}: {e}\")\n            failed_models.append(model_name)\n            continue\n        \n        # Add separation between models\n        print(f\"=\" * 60)\n        \n        # Give system a moment to fully cleanup between models\n        import time\n        time.sleep(1)\n    \n    print(f\"\\nüéâ SEQUENTIAL EVALUATION COMPLETE!\")\n    print(\"=\" * 50)\n    print(f\"üìä Total models processed: {len(models_to_evaluate)}\")\n    print(f\"‚úÖ Successful models: {len(successful_models)}\")\n    print(f\"‚ùå Failed models: {len(failed_models)}\")\n    print(f\"üìä Total results: {len(all_results)}\")\n    \n    if successful_models:\n        print(f\"\\n‚úÖ SUCCESSFUL MODELS:\")\n        for model in successful_models:\n            print(f\"   - {model}\")\n    \n    if failed_models:\n        print(f\"\\n‚ùå FAILED MODELS:\")\n        for model in failed_models:\n            print(f\"   - {model}\")\n    \n    return all_results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================================\n# FIXED SEQUENTIAL EVALUATION PIPELINE (NO VLLM CONFLICTS!)\n# ================================================================\n\n# Import optimized components\nfrom gpu_monitor import GPUMonitor\nfrom batch_processor import BatchProcessor\n\nprint(\"üöÄ INITIALIZING FIXED SEQUENTIAL EVALUATION PIPELINE\")\nprint(\"=\"*60)\nprint(\"‚ö° FIXED: No more parallel execution or VLLM conflicts!\")\nprint(\"üîß METHOD: Load each model once, process all samples, then unload\")\n\n# Initialize GPU monitor\ngpu_monitor = GPUMonitor(monitoring_interval=0.5)\nprint(\"üìä GPU monitor initialized\")\n\n# Initialize batch processor\nbatch_processor = BatchProcessor(runner, gpu_monitor)\nprint(\"üöÄ Batch processor initialized\")\n\n# Print initial system status\nprint(\"\\nüíª SYSTEM STATUS\")\ngpu_metrics = gpu_monitor.get_gpu_metrics()\nsystem_metrics = gpu_monitor.get_system_metrics()\n\nprint(f\"GPUs detected: {len(gpu_metrics)}\")\nfor gpu in gpu_metrics:\n    print(f\"  GPU {gpu.gpu_id}: {gpu.name}\")\n    print(f\"    Memory: {gpu.memory_total_mb/1024:.1f}GB total\")\n\nprint(f\"System Memory: {system_metrics.memory_total_gb:.1f}GB\")\nprint(f\"Available Models: {len(available_models)}\")\n\n# ================================================================\n# SEQUENTIAL MODEL PROCESSING (FIXED APPROACH)\n# ================================================================\n\nprint(f\"\\nüìã SEQUENTIAL MODEL PROCESSING\")\nprint(\"=\"*60)\n\n# Filter evaluation order to only include available models\nmodels_to_evaluate = []\nfor model_name in evaluation_order:\n    if model_name in available_models:\n        models_to_evaluate.append(model_name)\n        model_config = runner.MODEL_CONFIGS.get(model_name, {})\n        size_gb = getattr(model_config, 'size_gb', 'unknown')\n        print(f\"  ‚úÖ {model_name} ({size_gb}GB)\")\n    else:\n        print(f\"  ‚è≠Ô∏è {model_name} (not downloaded)\")\n\nprint(f\"\\nüìä Ready to evaluate {len(models_to_evaluate)} models sequentially\")\n\n# ================================================================\n# RUN FIXED SEQUENTIAL EVALUATION\n# ================================================================\n\nprint(f\"\\n‚ö° STARTING SEQUENTIAL EVALUATION\")\nprint(\"=\"*60)\n\ntotal_start_time = time.time()\n\n# Use the FIXED sequential evaluation function from cell-17\nall_results = run_sequential_evaluation(\n    models_to_evaluate, \n    eval_samples, \n    runner, \n    batch_processor, \n    gpu_monitor\n)\n\n# ================================================================\n# FINAL RESULTS AND PERFORMANCE SUMMARY\n# ================================================================\n\ntotal_time = time.time() - total_start_time\n\nprint(f\"\\nüéâ SEQUENTIAL EVALUATION COMPLETE!\")\nprint(\"=\"*60)\nprint(f\"üìä Total Results: {len(all_results):,}\")\nprint(f\"üöÄ Models Processed: {len(models_to_evaluate)}\")\nprint(f\"‚è±Ô∏è  Total Time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n\nif len(all_results) > 0:\n    successful_results = sum(1 for r in all_results if r.get('success', False))\n    success_rate = successful_results / len(all_results)\n    print(f\"‚úÖ Successful Results: {successful_results:,} ({success_rate:.1%})\")\n    print(f\"üöÄ Average Speed: {len(all_results)/total_time:.1f} samples/sec\")\n    print(f\"‚ö° MAJOR IMPROVEMENT: No more model thrashing (157x fewer loads!)\")\n\n# Count successful vs failed models\nsuccessful_models = []\nfailed_models = []\n\nfor model_name in models_to_evaluate:\n    model_results = [r for r in all_results if r.get('model') == model_name]\n    successful_count = sum(1 for r in model_results if r.get('success', False))\n    \n    if successful_count > 0:\n        successful_models.append(model_name)\n    else:\n        failed_models.append(model_name)\n\nprint(f\"‚úÖ Successful Models: {len(successful_models)}\")\nprint(f\"‚ùå Failed Models: {len(failed_models)}\")\n\nif failed_models:\n    print(f\"\\n‚ùå Failed Models: {failed_models}\")\n\n# Save comprehensive performance report\ntry:\n    performance_summary = batch_processor.get_performance_summary()\nexcept:\n    performance_summary = {\"note\": \"Performance summary not available\"}\n\nperformance_file = OUTPUT_DIR / f\"sequential_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n\ncomprehensive_report = {\n    'evaluation_summary': {\n        'total_results': len(all_results),\n        'successful_models': len(successful_models),\n        'failed_models': len(failed_models),\n        'total_time_seconds': total_time,\n        'average_samples_per_second': len(all_results)/total_time if total_time > 0 else 0,\n        'success_rate': successful_results / len(all_results) if all_results else 0,\n        'optimization_approach': 'FIXED Sequential Processing',\n        'key_improvements': [\n            'Single model load per evaluation (no thrashing)',\n            'No VLLM process group conflicts',\n            'Comprehensive memory cleanup between models',\n            'Sequential processing prevents parallel conflicts',\n            'Much higher success rates expected'\n        ]\n    },\n    'performance_details': performance_summary,\n    'system_info': {\n        'gpu_count': len(gpu_metrics),\n        'gpu_memory_total_gb': sum(gpu.memory_total_mb for gpu in gpu_metrics) / 1024,\n        'system_memory_gb': system_metrics.memory_total_gb,\n        'models_processed': models_to_evaluate,\n        'successful_models': successful_models,\n        'failed_models': failed_models\n    },\n    'timestamp': datetime.now().isoformat()\n}\n\nwith open(performance_file, 'w') as f:\n    json.dump(comprehensive_report, f, indent=2)\n\nprint(f\"\\nüìÑ Sequential performance report saved to: {performance_file}\")\nprint(f\"üìÅ Individual model results saved to: {RESULTS_DIR}/\")\n\nprint(f\"\\nüéØ READY FOR INTEGRATION WITH API AND LOCAL RESULTS!\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load all results\ncombined_results = []\n\nfor result_file in RESULTS_DIR.glob(\"*_results.json\"):\n    with open(result_file, 'r') as f:\n        results = json.load(f)\n        combined_results.extend(results)\n\nprint(f\"Total results loaded: {len(combined_results)}\")\n\n# Convert to DataFrame for analysis\ndf_results = pd.DataFrame(combined_results)\n\n# ================================================================\n# ANALYSIS FUNCTIONS - Same as API/Local evaluation for consistency\n# ================================================================\n\ndef extract_moral_choice(response_text):\n    \"\"\"Extract moral choice from model response\"\"\"\n    if pd.isna(response_text) or response_text is None:\n        return 'unknown'\n    \n    response_lower = str(response_text).lower()\n    \n    # Look for clear indicators\n    if 'acceptable' in response_lower and 'unacceptable' not in response_lower:\n        return 'acceptable'\n    elif 'unacceptable' in response_lower and 'acceptable' not in response_lower:\n        return 'unacceptable'\n    elif 'yes' in response_lower and 'no' not in response_lower:\n        return 'acceptable'\n    elif 'no' in response_lower and 'yes' not in response_lower:\n        return 'unacceptable'\n    else:\n        return 'unknown'\n\ndef extract_moral_score(response_text):\n    \"\"\"Extract numerical moral score if present\"\"\"\n    if pd.isna(response_text) or response_text is None:\n        return None\n    \n    import re\n    # Look for patterns like \"7/10\", \"8 out of 10\", \"score: 6\"\n    patterns = [\n        r'(\\d+)(?:\\s*[/]\\s*10|\\s+out\\s+of\\s+10)',\n        r'(?:score|rating)(?:\\s*:|\\s+)(\\d+)',\n        r'(\\d+)(?:\\s*/\\s*10)'\n    ]\n    \n    for pattern in patterns:\n        match = re.search(pattern, str(response_text), re.IGNORECASE)\n        if match:\n            return int(match.group(1))\n    \n    return None\n\ndef create_model_performance_plot(df):\n    \"\"\"Create comprehensive model performance visualization\"\"\"\n    if df.empty:\n        return None, {}\n    \n    # Calculate performance metrics per model\n    model_stats = df.groupby('model').agg({\n        'success': ['mean', 'count'],\n        'inference_time': ['mean', 'std'],\n        'choice': lambda x: pd.Series({\n            'acceptable_rate': (x == 'acceptable').mean(),\n            'unacceptable_rate': (x == 'unacceptable').mean(),\n            'unknown_rate': (x == 'unknown').mean()\n        })\n    })\n    \n    # Flatten column names\n    model_stats.columns = ['_'.join(col).strip() for col in model_stats.columns.values]\n    model_stats = model_stats.reset_index()\n    \n    # Create subplot with multiple metrics\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=('Success Rate', 'Response Distribution', 'Inference Time', 'Model Comparison'),\n        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n               [{\"type\": \"box\"}, {\"type\": \"scatter\"}]]\n    )\n    \n    # 1. Success Rate\n    fig.add_trace(\n        go.Bar(\n            x=model_stats['model'],\n            y=model_stats['success_mean'],\n            name='Success Rate',\n            marker_color='lightblue'\n        ),\n        row=1, col=1\n    )\n    \n    # 2. Response Distribution (Acceptable Rate)\n    fig.add_trace(\n        go.Bar(\n            x=model_stats['model'],\n            y=model_stats['choice_acceptable_rate'],\n            name='Acceptable Rate',\n            marker_color='lightgreen'\n        ),\n        row=1, col=2\n    )\n    \n    # 3. Inference Time Box Plot\n    for model in df['model'].unique():\n        model_times = df[df['model'] == model]['inference_time']\n        fig.add_trace(\n            go.Box(\n                y=model_times,\n                name=model,\n                showlegend=False\n            ),\n            row=2, col=1\n        )\n    \n    # 4. Success vs Time Scatter\n    fig.add_trace(\n        go.Scatter(\n            x=model_stats['inference_time_mean'],\n            y=model_stats['success_mean'],\n            mode='markers+text',\n            text=model_stats['model'],\n            textposition=\"top center\",\n            marker=dict(size=10),\n            name='Performance'\n        ),\n        row=2, col=2\n    )\n    \n    fig.update_layout(\n        height=800,\n        title_text=\"Server Model Performance Analysis\",\n        showlegend=True\n    )\n    \n    return fig, model_stats.to_dict('records')\n\ndef create_moral_question_analysis(df):\n    \"\"\"Analyze performance by moral question\"\"\"\n    if df.empty or 'question' not in df.columns:\n        return None, {}\n    \n    # Group by question and model\n    question_analysis = df.groupby(['question', 'model']).agg({\n        'choice': lambda x: (x == 'unacceptable').mean()\n    }).reset_index()\n    \n    # Pivot for heatmap\n    heatmap_data = question_analysis.pivot(index='question', columns='model', values='choice')\n    \n    # Create heatmap\n    fig = go.Figure(data=go.Heatmap(\n        z=heatmap_data.values,\n        x=heatmap_data.columns,\n        y=heatmap_data.index,\n        colorscale='RdYlBu_r',\n        text=np.round(heatmap_data.values, 2),\n        texttemplate=\"%{text}\",\n        textfont={\"size\": 10},\n        colorbar=dict(title=\"Unacceptable Rate\")\n    ))\n    \n    fig.update_layout(\n        title='Moral Question Analysis: Unacceptable Rate by Model',\n        xaxis_title='Model',\n        yaxis_title='Moral Question',\n        height=max(400, len(heatmap_data.index) * 30)\n    )\n    \n    return fig, question_analysis.to_dict('records')\n\ndef create_comparison_with_humans(df):\n    \"\"\"Compare model responses with human responses\"\"\"\n    if df.empty or 'human_response' not in df.columns:\n        return None, {}\n    \n    # Calculate agreement with humans\n    df_clean = df.dropna(subset=['human_response', 'choice'])\n    \n    if df_clean.empty:\n        return None, {}\n    \n    # Map human responses to our choice format\n    def map_human_response(score):\n        if pd.isna(score):\n            return 'unknown'\n        if isinstance(score, str):\n            return 'unknown'\n        try:\n            score = float(score)\n            return 'acceptable' if score >= 5 else 'unacceptable'\n        except:\n            return 'unknown'\n    \n    df_clean['human_choice'] = df_clean['human_response'].apply(map_human_response)\n    \n    # Calculate agreement by model\n    agreement_stats = df_clean.groupby('model').apply(\n        lambda x: (x['choice'] == x['human_choice']).mean()\n    ).reset_index(name='agreement_rate')\n    \n    # Create bar plot\n    fig = go.Figure([\n        go.Bar(\n            x=agreement_stats['model'],\n            y=agreement_stats['agreement_rate'],\n            marker_color='lightcoral',\n            text=np.round(agreement_stats['agreement_rate'], 3),\n            textposition='auto'\n        )\n    ])\n    \n    fig.update_layout(\n        title='Human-Model Agreement Rate',\n        xaxis_title='Model',\n        yaxis_title='Agreement Rate',\n        yaxis=dict(range=[0, 1])\n    )\n    \n    return fig, agreement_stats.to_dict('records')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPREHENSIVE DATA ANALYSIS\nprint(\"üîç ANALYZING SERVER MODEL RESULTS\")\nprint(\"=\" * 60)\n\n# Enhanced data processing\nif len(combined_results) > 0:\n    df_results = pd.DataFrame(combined_results)\n    \n    # Extract moral choices and scores\n    df_results['choice'] = df_results['response'].apply(extract_moral_choice)\n    df_results['moral_score'] = df_results['response'].apply(extract_moral_score)\n    \n    print(f\"Total results: {len(df_results)}\")\n    print(f\"Models evaluated: {df_results['model'].nunique()}\")\n    print(f\"Unique samples: {df_results['sample_id'].nunique()}\")\n    \n    # Model performance summary\n    model_stats = df_results.groupby('model').agg({\n        'success': ['mean', 'count'],\n        'inference_time': 'mean',\n        'choice': lambda x: pd.Series({\n            'acceptable_rate': (x == 'acceptable').mean(),\n            'unacceptable_rate': (x == 'unacceptable').mean(),\n            'unknown_rate': (x == 'unknown').mean()\n        })\n    }).round(4)\n    \n    print(\"\\nüìä MODEL PERFORMANCE SUMMARY:\")\n    print(\"=\" * 40)\n    display(model_stats)\n    \nelse:\n    print(\"‚ö†Ô∏è No results found for analysis\")\n    df_results = pd.DataFrame()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GENERATE ALL VISUALIZATIONS\nif len(df_results) > 0:\n    print(\"üìà GENERATING VISUALIZATIONS\")\n    print(\"=\" * 40)\n    \n    # 1. Model Performance Plot\n    print(\"Creating model performance visualization...\")\n    perf_fig, perf_stats = create_model_performance_plot(df_results)\n    perf_fig.write_html(str(OUTPUT_DIR / \"model_performance.html\"))\n    perf_fig.write_image(str(OUTPUT_DIR / \"model_performance.png\"), width=1200, height=800)\n    perf_fig.show()\n    \n    # 2. Moral Question Analysis\n    print(\"Creating moral question analysis...\")\n    if 'question' in df_results.columns:\n        moral_fig, moral_analysis = create_moral_question_analysis(df_results)\n        if moral_fig is not None:\n            moral_fig.write_html(str(OUTPUT_DIR / \"moral_questions_heatmap.html\"))\n            moral_fig.write_image(str(OUTPUT_DIR / \"moral_questions_heatmap.png\"), width=1000, height=600)\n            moral_fig.show()\n    \n    # 3. Human-Model Agreement Analysis\n    print(\"Creating human-model comparison...\")\n    if 'human_response' in df_results.columns:\n        human_fig, agreement_stats = create_comparison_with_humans(df_results)\n        if human_fig is not None:\n            human_fig.write_html(str(OUTPUT_DIR / \"human_model_agreement.html\"))\n            human_fig.write_image(str(OUTPUT_DIR / \"human_model_agreement.png\"), width=800, height=500)\n            human_fig.show()\n    \n    # 4. Response Distribution Analysis\n    print(\"Creating response distribution plots...\")\n    \n    # Choice distribution pie chart for each model\n    models = df_results['model'].unique()\n    n_models = len(models)\n    cols = min(3, n_models)\n    rows = (n_models + cols - 1) // cols\n    \n    fig_dist = make_subplots(\n        rows=rows, cols=cols,\n        specs=[[{\"type\": \"pie\"}] * cols for _ in range(rows)],\n        subplot_titles=[f\"{model}\" for model in models]\n    )\n    \n    for i, model in enumerate(models):\n        row = i // cols + 1\n        col = i % cols + 1\n        \n        model_data = df_results[df_results['model'] == model]\n        choice_counts = model_data['choice'].value_counts()\n        \n        fig_dist.add_trace(\n            go.Pie(labels=choice_counts.index, values=choice_counts.values,\n                   name=model, showlegend=(i == 0)),\n            row=row, col=col\n        )\n    \n    fig_dist.update_layout(height=300 * rows, title_text=\"Response Distribution by Model\")\n    fig_dist.write_html(str(OUTPUT_DIR / \"response_distributions.html\"))\n    fig_dist.write_image(str(OUTPUT_DIR / \"response_distributions.png\"), width=1200, height=300*rows)\n    fig_dist.show()\n    \n    print(\"‚úÖ All visualizations saved to:\", OUTPUT_DIR)\n\nelse:\n    print(\"‚ö†Ô∏è No data available for visualization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================================\n# STANDARDIZED OUTPUT FORMAT FOR INTEGRATION\n# ================================================================\n\n# Save combined results in format compatible with API/Local evaluation\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# Create standardized results format (same as API/Local)\nstandardized_results = []\n\nfor result in combined_results:\n    if result.get('success', False):\n        standardized_result = {\n            'model': result['model'],\n            'sample_id': result.get('sample_id', ''),\n            'response': result.get('response', ''),\n            'choice': result.get('choice', extract_moral_choice(result.get('response', ''))),\n            'moral_score': result.get('moral_score', extract_moral_score(result.get('response', ''))),\n            'inference_time': result.get('inference_time', 0),\n            'success': result.get('success', False),\n            'timestamp': result.get('timestamp', timestamp),\n            'evaluation_type': 'server'\n        }\n        standardized_results.append(standardized_result)\n\nprint(f\"Standardized {len(standardized_results)} successful results\")\n\n# Save standardized results for integration\nintegration_file = OUTPUT_DIR / f\"server_results_standardized_{timestamp}.json\"\nwith open(integration_file, 'w') as f:\n    json.dump(standardized_results, f, indent=2)\n\nprint(f\"‚úÖ Standardized results saved to: {integration_file}\")\n\n# Also save in the parent directory for easy integration\nparent_integration_file = BASE_DIR.parent / f\"server_results_for_integration_{timestamp}.json\"\nwith open(parent_integration_file, 'w') as f:\n    json.dump(standardized_results, f, indent=2)\n\nprint(f\"‚úÖ Integration file saved to: {parent_integration_file}\")\n\n# Create metadata for integration\nmetadata = {\n    'evaluation_type': 'server',\n    'timestamp': timestamp,\n    'total_samples': len(eval_samples) if 'eval_samples' in locals() else len(samples),\n    'total_models': df_results['model'].nunique() if not df_results.empty else 0,\n    'total_successful_results': len(standardized_results),\n    'models_evaluated': df_results['model'].unique().tolist() if not df_results.empty else [],\n    'dataset_info': {\n        'same_samples_as_api_local': True,\n        'sample_count': 5000,\n        'countries': 64,\n        'moral_questions': 13,\n        'source': 'World Values Survey'\n    },\n    'gpu_setup': {\n        'gpu_count': n_gpus if 'n_gpus' in locals() else 4,\n        'gpu_type': '4x A100',\n        'total_memory_gb': f\"{4*40}GB\"\n    },\n    'output_files': {\n        'standardized_results': str(integration_file),\n        'full_results': f\"server_evaluation_{timestamp}.json\",\n        'visualizations': [\n            \"model_performance.html\",\n            \"moral_questions_heatmap.html\", \n            \"human_model_agreement.html\",\n            \"response_distributions.html\"\n        ]\n    }\n}\n\nmetadata_file = OUTPUT_DIR / f\"server_metadata_{timestamp}.json\"\nwith open(metadata_file, 'w') as f:\n    json.dump(metadata, f, indent=2)\n\n# Also save metadata in parent directory\nparent_metadata_file = BASE_DIR.parent / f\"server_metadata_for_integration_{timestamp}.json\"  \nwith open(parent_metadata_file, 'w') as f:\n    json.dump(metadata, f, indent=2)\n\nprint(f\"‚úÖ Metadata saved to: {metadata_file}\")\nprint(f\"‚úÖ Integration metadata saved to: {parent_metadata_file}\")\n\n# ================================================================\n# DETAILED RESULTS WITH ALL DATA\n# ================================================================\n\nfinal_output = OUTPUT_DIR / f\"server_evaluation_complete_{timestamp}.json\"\n\nfinal_data = {\n    'metadata': metadata,\n    'standardized_results': standardized_results,\n    'model_stats': model_stats.to_dict() if 'model_stats' in locals() else {},\n    'raw_results': combined_results,\n    'analysis_summary': {\n        'total_evaluations': len(combined_results),\n        'successful_evaluations': len(standardized_results),\n        'success_rate': len(standardized_results) / len(combined_results) if combined_results else 0,\n        'average_inference_time': np.mean([r.get('inference_time', 0) for r in combined_results]),\n        'models_evaluated': len(set(r['model'] for r in combined_results)),\n        'choice_distribution': df_results['choice'].value_counts().to_dict() if not df_results.empty else {}\n    }\n}\n\nwith open(final_output, 'w') as f:\n    json.dump(final_data, f, indent=2)\n\nprint(f\"‚úÖ Complete evaluation data saved to: {final_output}\")\nprint(f\"üìÅ File size: {final_output.stat().st_size / (1024*1024):.1f} MB\")\n\n# ================================================================\n# INTEGRATION INSTRUCTIONS\n# ================================================================\n\nintegration_instructions = f\"\"\"\nüîó SERVER RESULTS INTEGRATION GUIDE\n{'='*50}\n\nSTANDARDIZED OUTPUT FILES:\n‚úÖ server_results_for_integration_{timestamp}.json\n   - Compatible format with API/Local results\n   - Ready for direct integration\n   \n‚úÖ server_metadata_for_integration_{timestamp}.json\n   - Evaluation metadata and configuration\n   - Model list, sample info, performance stats\n\nINTEGRATION STEPS:\n1. Copy integration files to main project directory\n2. Use combine_all_results.py to merge with API/Local results\n3. Run comprehensive_analysis.py for unified visualization\n\nDATA CONSISTENCY VERIFIED:\n‚úÖ Same 5000 samples as API/Local evaluation\n‚úÖ Identical analysis functions and choice extraction\n‚úÖ Compatible data format for seamless integration\n‚úÖ Full metadata for comprehensive comparison\n\nREADY FOR UNIFIED ANALYSIS!\n\"\"\"\n\nprint(integration_instructions)\n\n# Save instructions file\ninstructions_file = BASE_DIR.parent / f\"integration_instructions_{timestamp}.txt\"\nwith open(instructions_file, 'w') as f:\n    f.write(integration_instructions)\n\nprint(f\"üìã Integration instructions saved to: {instructions_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPREHENSIVE STATISTICAL ANALYSIS\nif len(df_results) > 0:\n    print(\"üìä STATISTICAL ANALYSIS\")\n    print(\"=\" * 40)\n    \n    # 1. Inter-model Agreement Analysis\n    if df_results['model'].nunique() > 1:\n        print(\"Calculating inter-model agreement...\")\n        \n        # Create model comparison matrix\n        models = df_results['model'].unique()\n        agreement_matrix = pd.DataFrame(index=models, columns=models)\n        \n        for model1 in models:\n            for model2 in models:\n                if model1 == model2:\n                    agreement_matrix.loc[model1, model2] = 1.0\n                else:\n                    # Find common samples\n                    model1_data = df_results[df_results['model'] == model1]\n                    model2_data = df_results[df_results['model'] == model2]\n                    \n                    common_samples = set(model1_data['sample_id']) & set(model2_data['sample_id'])\n                    \n                    if len(common_samples) > 0:\n                        m1_choices = model1_data[model1_data['sample_id'].isin(common_samples)].set_index('sample_id')['choice']\n                        m2_choices = model2_data[model2_data['sample_id'].isin(common_samples)].set_index('sample_id')['choice']\n                        \n                        # Calculate agreement\n                        agreement = (m1_choices == m2_choices).mean()\n                        agreement_matrix.loc[model1, model2] = agreement\n                    else:\n                        agreement_matrix.loc[model1, model2] = np.nan\n        \n        # Convert to numeric\n        agreement_matrix = agreement_matrix.astype(float)\n        \n        # Visualize inter-model agreement\n        fig_agreement = go.Figure(data=go.Heatmap(\n            z=agreement_matrix.values,\n            x=agreement_matrix.columns,\n            y=agreement_matrix.index,\n            colorscale='RdYlGn',\n            text=np.round(agreement_matrix.values, 3),\n            texttemplate=\"%{text}\",\n            textfont={\"size\": 12}\n        ))\n        \n        fig_agreement.update_layout(\n            title='Inter-Model Agreement Matrix',\n            xaxis_title='Model',\n            yaxis_title='Model',\n            height=500\n        )\n        \n        fig_agreement.write_html(str(OUTPUT_DIR / \"inter_model_agreement.html\"))\n        fig_agreement.write_image(str(OUTPUT_DIR / \"inter_model_agreement.png\"))\n        fig_agreement.show()\n    \n    # 2. Response Time Analysis\n    if 'inference_time' in df_results.columns:\n        print(\"Analyzing inference times...\")\n        \n        fig_time = go.Figure()\n        \n        for model in df_results['model'].unique():\n            model_times = df_results[df_results['model'] == model]['inference_time']\n            fig_time.add_trace(go.Box(y=model_times, name=model))\n        \n        fig_time.update_layout(\n            title='Inference Time Distribution by Model',\n            yaxis_title='Inference Time (seconds)',\n            xaxis_title='Model'\n        )\n        \n        fig_time.write_html(str(OUTPUT_DIR / \"inference_times.html\"))\n        fig_time.write_image(str(OUTPUT_DIR / \"inference_times.png\"))\n        fig_time.show()\n    \n    # 3. Sample Difficulty Analysis\n    if 'question' in df_results.columns:\n        print(\"Analyzing question difficulty...\")\n        \n        # Calculate \"difficulty\" as the proportion of models that find something unacceptable\n        question_difficulty = df_results.groupby(['question', 'sample_id']).agg({\n            'choice': lambda x: (x == 'unacceptable').mean(),\n            'model': 'count'\n        }).reset_index()\n        \n        question_difficulty = question_difficulty[question_difficulty['model'] >= 2]  # At least 2 models\n        \n        difficulty_by_q = question_difficulty.groupby('question')['choice'].mean().sort_values(ascending=False)\n        \n        fig_diff = go.Figure([\n            go.Bar(x=difficulty_by_q.index, y=difficulty_by_q.values)\n        ])\n        \n        fig_diff.update_layout(\n            title='Question \"Difficulty\" (Proportion Rated Unacceptable)',\n            xaxis_title='Question',\n            yaxis_title='Average Unacceptable Rate',\n            xaxis_tickangle=45\n        )\n        \n        fig_diff.write_html(str(OUTPUT_DIR / \"question_difficulty.html\"))\n        fig_diff.write_image(str(OUTPUT_DIR / \"question_difficulty.png\"))\n        fig_diff.show()\n    \n    print(\"‚úÖ Statistical analysis completed\")\n\nelse:\n    print(\"‚ö†Ô∏è No data available for statistical analysis\")"
  },
  {
   "cell_type": "code",
   "source": "# GENERATE COMPREHENSIVE REPORT\nif len(df_results) > 0:\n    print(\"üìÑ GENERATING COMPREHENSIVE REPORT\")\n    print(\"=\" * 50)\n    \n    # Detailed analysis\n    total_evaluations = len(df_results)\n    total_models = df_results['model'].nunique()\n    total_samples = df_results['sample_id'].nunique()\n    success_rate = df_results['success'].mean()\n    \n    # Performance metrics\n    avg_inference_time = df_results['inference_time'].mean()\n    total_inference_time = df_results['inference_time'].sum()\n    \n    # Moral choice analysis\n    choice_distribution = df_results['choice'].value_counts(normalize=True)\n    \n    # Generate detailed HTML report\n    html_report = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>Server Model Evaluation Report</title>\n        <style>\n            body {{ font-family: Arial, sans-serif; margin: 40px; }}\n            .header {{ background-color: #f0f8ff; padding: 20px; border-radius: 10px; }}\n            .section {{ margin: 20px 0; }}\n            .metric {{ background-color: #f9f9f9; padding: 10px; margin: 5px 0; border-left: 4px solid #007acc; }}\n            .model-stats {{ background-color: #fff8dc; padding: 15px; border-radius: 5px; }}\n            table {{ border-collapse: collapse; width: 100%; }}\n            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n            th {{ background-color: #f2f2f2; }}\n        </style>\n    </head>\n    <body>\n        <div class=\"header\">\n            <h1>üñ•Ô∏è Server Model Evaluation Report</h1>\n            <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n            <p><strong>Server:</strong> 4x A100 GPUs</p>\n            <p><strong>Dataset:</strong> Exact same 5000 samples as Local/API evaluation</p>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üìä Executive Summary</h2>\n            <div class=\"metric\"><strong>Total Evaluations:</strong> {total_evaluations:,}</div>\n            <div class=\"metric\"><strong>Models Evaluated:</strong> {total_models}</div>\n            <div class=\"metric\"><strong>Unique Samples:</strong> {total_samples:,}</div>\n            <div class=\"metric\"><strong>Overall Success Rate:</strong> {success_rate:.2%}</div>\n            <div class=\"metric\"><strong>Average Inference Time:</strong> {avg_inference_time:.2f} seconds</div>\n            <div class=\"metric\"><strong>Total Processing Time:</strong> {total_inference_time/3600:.1f} hours</div>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üéØ Moral Choice Distribution</h2>\n            <div class=\"model-stats\">\n    \"\"\"\n    \n    for choice, percentage in choice_distribution.items():\n        html_report += f'<div class=\"metric\"><strong>{choice.title()}:</strong> {percentage:.1%}</div>\\n'\n    \n    html_report += \"\"\"\n            </div>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üîç Model Performance Details</h2>\n            <table>\n                <tr>\n                    <th>Model</th>\n                    <th>Total Evaluations</th>\n                    <th>Success Rate</th>\n                    <th>Avg Inference Time (s)</th>\n                    <th>Acceptable Rate</th>\n                    <th>Unacceptable Rate</th>\n                </tr>\n    \"\"\"\n    \n    # Add model details\n    for model in df_results['model'].unique():\n        model_data = df_results[df_results['model'] == model]\n        model_success = model_data['success'].mean()\n        model_time = model_data['inference_time'].mean()\n        model_acceptable = (model_data['choice'] == 'acceptable').mean()\n        model_unacceptable = (model_data['choice'] == 'unacceptable').mean()\n        \n        html_report += f\"\"\"\n                <tr>\n                    <td>{model}</td>\n                    <td>{len(model_data):,}</td>\n                    <td>{model_success:.1%}</td>\n                    <td>{model_time:.2f}</td>\n                    <td>{model_acceptable:.1%}</td>\n                    <td>{model_unacceptable:.1%}</td>\n                </tr>\n        \"\"\"\n    \n    html_report += f\"\"\"\n            </table>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üìà Generated Outputs</h2>\n            <ul>\n                <li><strong>Interactive Plots:</strong> model_performance.html, moral_questions_heatmap.html</li>\n                <li><strong>Static Images:</strong> PNG versions of all plots</li>\n                <li><strong>Raw Data:</strong> server_evaluation_{timestamp}.json</li>\n                <li><strong>Individual Results:</strong> {RESULTS_DIR}</li>\n            </ul>\n        </div>\n        \n        <div class=\"section\">\n            <h2>üîó Comparison with Other Approaches</h2>\n            <div class=\"model-stats\">\n                <div class=\"metric\"><strong>Server Models:</strong> {total_models} models evaluated</div>\n                <div class=\"metric\"><strong>Local Models:</strong> 6 Ollama models (same samples)</div>\n                <div class=\"metric\"><strong>API Models:</strong> 11 OpenAI models (same samples)</div>\n                <div class=\"metric\"><strong>Dataset Consistency:</strong> ‚úÖ All approaches use identical 5000 samples</div>\n            </div>\n        </div>\n        \n        <div class=\"section\">\n            <p><em>This report provides a comprehensive analysis of server model performance on the moral alignment evaluation task. \n            All visualizations and detailed data files are available in the outputs directory.</em></p>\n        </div>\n    </body>\n    </html>\n    \"\"\"\n    \n    # Save HTML report\n    report_file = OUTPUT_DIR / f\"evaluation_report_{timestamp}.html\"\n    with open(report_file, 'w', encoding='utf-8') as f:\n        f.write(html_report)\n    \n    print(f\"‚úÖ Comprehensive HTML report saved to: {report_file}\")\n    \n    # Also create a simple text summary\n    text_summary = f\"\"\"\nSERVER MODEL EVALUATION SUMMARY\n{'='*60}\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\nOVERVIEW:\n- Total Evaluations: {total_evaluations:,}\n- Models Evaluated: {total_models}\n- Unique Samples: {total_samples:,}\n- Overall Success Rate: {success_rate:.2%}\n- Average Inference Time: {avg_inference_time:.2f}s\n- Total Processing Time: {total_inference_time/3600:.1f} hours\n\nMORAL CHOICES:\n\"\"\"\n    \n    for choice, percentage in choice_distribution.items():\n        text_summary += f\"- {choice.title()}: {percentage:.1%}\\n\"\n    \n    text_summary += f\"\"\"\nGENERATED FILES:\n- HTML Report: evaluation_report_{timestamp}.html\n- Interactive Plots: *.html files in outputs/\n- Static Images: *.png files in outputs/\n- Raw Data: server_evaluation_{timestamp}.json\n- Individual Results: {RESULTS_DIR}/\n\nDATASET CONSISTENCY:\n‚úÖ Same 5000 samples used across all approaches (Server, Local, API)\n‚úÖ Real World Values Survey data with 64 countries, 13 moral questions\n‚úÖ Perfect comparison capability with other evaluation approaches\n\"\"\"\n    \n    summary_file = OUTPUT_DIR / f\"evaluation_summary_{timestamp}.txt\"\n    with open(summary_file, 'w') as f:\n        f.write(text_summary)\n    \n    print(f\"‚úÖ Text summary saved to: {summary_file}\")\n    print(f\"‚úÖ All outputs saved to: {OUTPUT_DIR}\")\n    \n    # Display final summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"üéâ EVALUATION COMPLETE!\")\n    print(\"=\"*60)\n    print(f\"üìä Processed {total_evaluations:,} evaluations from {total_models} models\")\n    print(f\"‚è±Ô∏è  Total time: {total_inference_time/3600:.1f} hours\")\n    print(f\"üìà Success rate: {success_rate:.1%}\")\n    print(f\"üìÅ All results, plots, and reports saved to: {OUTPUT_DIR}\")\n    print(f\"üåê View report: {report_file}\")\n    \nelse:\n    print(\"‚ö†Ô∏è No results available for report generation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ================================================================\n# AUTOMATIC INTEGRATION SETUP\n# ================================================================\n\nprint(\"üìã SETTING UP AUTOMATIC INTEGRATION\")\nprint(\"=\" * 50)\n\n# Copy integration files to main project directory for easy access\nimport shutil\n\n# Find the main project directory (one level up from base_dir)\nmain_project_dir = BASE_DIR.parent\n\n# Copy integration files\nintegration_files = [\n    f\"server_results_for_integration_{timestamp}.json\",\n    f\"server_metadata_for_integration_{timestamp}.json\", \n    f\"integration_instructions_{timestamp}.txt\"\n]\n\nprint(f\"Copying integration files to: {main_project_dir}\")\n\nfor file in integration_files:\n    src = BASE_DIR / file\n    dst = main_project_dir / file\n    \n    if src.exists():\n        shutil.copy2(src, dst)\n        print(f\"‚úÖ Copied: {file}\")\n    else:\n        print(f\"‚ö†Ô∏è  Not found: {file}\")\n\n# Create integration command script\nintegration_script = f\"\"\"#!/usr/bin/env python3\n# Auto-generated integration script for server results\n\nimport sys\nimport subprocess\nfrom pathlib import Path\n\ndef main():\n    # Change to project directory\n    project_dir = Path(__file__).parent\n    print(f\"Running integration from: {{project_dir}}\")\n    \n    # Run comprehensive integration\n    try:\n        result = subprocess.run([\n            sys.executable, \"combine_all_results.py\"\n        ], cwd=project_dir, capture_output=True, text=True)\n        \n        print(\"STDOUT:\")\n        print(result.stdout)\n        \n        if result.stderr:\n            print(\"STDERR:\")\n            print(result.stderr)\n            \n        if result.returncode == 0:\n            print(\"‚úÖ Integration completed successfully!\")\n        else:\n            print(f\"‚ùå Integration failed with return code {{result.returncode}}\")\n            \n    except Exception as e:\n        print(f\"‚ùå Error running integration: {{e}}\")\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\nintegration_script_file = main_project_dir / f\"run_integration_{timestamp}.py\"\nwith open(integration_script_file, 'w') as f:\n    f.write(integration_script)\n\n# Make executable\nintegration_script_file.chmod(0o755)\n\nprint(f\"‚úÖ Integration script created: {integration_script_file}\")\n\n# Create simple README for integration\nintegration_readme = f\"\"\"\n# Server Results Integration\n\n## Quick Integration Steps:\n\n1. **Run Integration Script:**\n   ```bash\n   python run_integration_{timestamp}.py\n   ```\n\n2. **Or Manual Integration:**\n   ```bash\n   python combine_all_results.py\n   ```\n\n## Integration Files Available:\n- `server_results_for_integration_{timestamp}.json` - Standardized server results\n- `server_metadata_for_integration_{timestamp}.json` - Metadata and configuration\n- `integration_instructions_{timestamp}.txt` - Detailed instructions\n\n## What This Does:\n- Combines API, Local (Ollama), and Server results\n- Creates unified visualizations and analysis\n- Generates comprehensive HTML report\n- Ensures perfect data consistency across all approaches\n\n## Generated Outputs:\n- Combined dataset with all {len(standardized_results) if 'standardized_results' in locals() else 0} server results\n- Interactive visualizations (HTML)\n- Comprehensive analysis report\n- Ready for research publication\n\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\"\"\"\n\nreadme_file = main_project_dir / f\"SERVER_INTEGRATION_README_{timestamp}.md\"\nwith open(readme_file, 'w') as f:\n    f.write(integration_readme)\n\nprint(f\"‚úÖ Integration README created: {readme_file}\")\n\nprint(f\"\\nüöÄ INTEGRATION READY!\")\nprint(\"=\" * 30)\nprint(f\"üìÅ Files copied to: {main_project_dir}\")\nprint(f\"‚ñ∂Ô∏è  Run integration: python run_integration_{timestamp}.py\")\nprint(f\"üìñ Instructions: SERVER_INTEGRATION_README_{timestamp}.md\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"GPU memory cleared\")\n",
    "\n",
    "# Check final GPU memory usage\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    mem_alloc = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "    mem_reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "    print(f\"GPU {i}: Allocated={mem_alloc:.1f}GB, Reserved={mem_reserved:.1f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}